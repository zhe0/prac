{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhe0/prac/blob/main/simple_rag_by_gguf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rag chatbot example\n",
        "* 偏小的語言模組\n",
        "* 減少幻覺程度"
      ],
      "metadata": {
        "id": "ZJbsrUlcYUAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 1. 定義 Transformer 自動編碼器模型\n",
        "# ------------------------------------\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self, feature_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding_encoder = nn.Linear(feature_size, d_model) # 輸入特徵 Embedding 層\n",
        "        self.embedding_decoder = nn.Linear(d_model, feature_size) # 輸出特徵 De-embedding 層 (用於重建)\n",
        "\n",
        "        # Encoder 層堆疊\n",
        "        encoder_layers = [TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_encoder_layers)]\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Decoder 層堆疊\n",
        "        decoder_layers = [TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_decoder_layers)]\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Encoder 部分\n",
        "        enc_src = self.embedding_encoder(src) # 將輸入特徵轉換到 d_model 維度 [batch_size, seq_len, d_model]\n",
        "        memory = self.encoder(enc_src)       # 通過 Encoder 層 [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Decoder 部分 (簡單的直接將 encoder 的 memory 作為 decoder 的輸入)\n",
        "        dec_output = self.decoder(memory, memory) # 通過 Decoder 層，memory 同時作為 decoder 的 tgt 和 memory [batch_size, seq_len, d_model]\n",
        "        output = self.embedding_decoder(dec_output) # 將 decoder 的輸出轉換回原始特徵維度 [batch_size, seq_len, feature_size]\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "QGye1ERE-e5m",
        "outputId": "c23afc14-144a-4531-ab8d-0d393b4f360f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.9/36.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F53QV6f6-I-l",
        "outputId": "539ba866-d60c-4ac1-a75b-5715f1a0e3c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0297f5ef483e4dc68fd7fbec460e3b31",
            "2180a0af632a404791a6cd01d9c29d72",
            "b22fe18e7f3d4adb9e9f379ed9ef70c1",
            "ae9752e84e38428b84785f62c2c98f5f",
            "7922b1396b4f459b84817f3726770e22",
            "432fbb6cb3a44a129acbfea14f1ca815",
            "3bd8b62f3431423ea9bd07503f73cfec",
            "e00e20e53af34d9285283d143168499c",
            "26b2b02d5a794d6292ea9e76a269f804",
            "83aec98c57594530ac2d9e069fb5c813",
            "eca48f6113c640cba713700571c05c55"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gemma-2b-it-q8_0.gguf:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0297f5ef483e4dc68fd7fbec460e3b31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 164 tensors from /root/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/gemma-2b-it-q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
            "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
            "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
            "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
            "llama_model_loader: - type  f32:   37 tensors\n",
            "llama_model_loader: - type q8_0:  127 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256128\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 8\n",
            "llm_load_print_meta: n_head_kv        = 1\n",
            "llm_load_print_meta: n_layer          = 18\n",
            "llm_load_print_meta: n_rot            = 256\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 16384\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 2B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 2.51 B\n",
            "llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = gemma-2b-it\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2539.93 MiB\n",
            ".............................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
            "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =     0.08 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     7.88 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.file_type': '7', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
            "Using fallback chat format: None\n",
            "\n",
            "llama_print_timings:        load time =    1860.65 ms\n",
            "llama_print_timings:      sample time =     274.71 ms /    54 runs   (    5.09 ms per token,   196.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11709.09 ms /    47 tokens (  249.13 ms per token,     4.01 tokens per second)\n",
            "llama_print_timings:        eval time =   24820.46 ms /    53 runs   (  468.31 ms per token,     2.14 tokens per second)\n",
            "llama_print_timings:       total time =   38822.58 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sure, here is the introduction and translation of the question and answer:\\n\\n**Question:** 你有沒有聽過「FUBON」在台灣的說法嗎？\\n\\n**Answer:** 我沒有聽過「FUBON」在台灣的說法。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# 2. 從 CSV 檔案載入資料並準備訓練/測試集\n",
        "# ------------------------------------\n",
        "def load_and_prepare_data(csv_file='transaction_data.csv', feature_cols=['V1', 'V2'], test_size=100):\n",
        "    \"\"\"\n",
        "    從 CSV 檔案載入資料，選擇特徵欄位，並分割成訓練集和測試集。\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): CSV 檔案路徑 (假設已上傳到 Colab).\n",
        "        feature_cols (list): 要使用的特徵欄位名稱列表.\n",
        "        test_size (int): 測試集大小 (取最後幾 rows 作為測試集).\n",
        "\n",
        "    Returns:\n",
        "        tuple: 訓練集 (torch.Tensor), 測試集 (torch.Tensor), 真實標籤 (測試集, torch.Tensor, 如果 CSV 包含 'Class' 欄位).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # 選擇特徵欄位\n",
        "    X = df[feature_cols].values\n",
        "    y = None # 預設沒有標籤\n",
        "    if 'Class' in df.columns:\n",
        "        y = df['Class'].values # 如果 CSV 包含 'Class' 欄位，則讀取標籤\n",
        "\n",
        "    # 將資料轉換成 PyTorch tensors\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    if y is not None:\n",
        "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "    else:\n",
        "        y_tensor = None\n",
        "\n",
        "    # 分割訓練集和測試集 (取最後 test_size rows 作為測試集)\n",
        "    train_data = X_tensor[:-test_size]\n",
        "    test_data = X_tensor[-test_size:]\n",
        "    test_labels = None\n",
        "    if y_tensor is not None:\n",
        "        test_labels = y_tensor[-test_size:]\n",
        "\n",
        "    return train_data, test_data, test_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 設定超參數和模型、優化器、損失函數\n",
        "# ------------------------------------\n",
        "feature_size = 2 # 輸入特徵維度 (V1, V2)\n",
        "sequence_length = 1 # 每個樣本視為長度為 1 的序列 (因為我們目前是獨立處理每個 row)\n",
        "d_model = 64       # Transformer 模型中的 embedding dimension\n",
        "nhead = 2          # Multi-head attention head 數量\n",
        "num_encoder_layers = 2 # Encoder 層數\n",
        "num_decoder_layers = 2 # Decoder 層數\n",
        "dim_feedforward = 128 # Feedforward network hidden layer dimension\n",
        "dropout = 0.1\n",
        "learning_rate = 0.001\n",
        "epochs = 30        # 增加 epochs 讓模型有更多訓練機會\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 檢查是否有 GPU 可用，有的話使用 GPU 加速\n",
        "\n",
        "model = TransformerAutoencoder(feature_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # 使用 Adam 優化器\n",
        "criterion = nn.MSELoss(reduction='mean') # 使用均方誤差 (MSE) 作為重建誤差損失函數\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fL2n0ve5AA9q",
        "outputId": "ce2bfab9-5091-4a0c-b4d2-f82153f62fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='1 \\n 金融業運用 人工智慧 (AI)之核心原則與相關推動 政策  \\n一、前言  \\n近來 AI1在金融服務 領域的應用日益增加， 為金融產業\\n提供客戶服務 帶來效益，亦同時衍生一些新的風險問題 及監\\n理挑戰。 為協助金融機構善用 AI科技優勢，並 能有效管理\\n風險、確保公平、保護消費者權益、維護系統安全及實現永\\n續發展，本會 依據行政院「 臺灣 AI行動計畫 2.0」政策規劃\\n及「數位政策法制協調 專案會議」之推動策略 ，並參考全球\\n主要國家監理機關及國際組織之相關指導原則， 及結合我國\\n金融市場發展狀況及本會監理政策方向，擬定適合我國金融\\n業的 6項AI應用核心原則，以 期引導金融業在兼顧消費者\\n權益、金融市場秩序及社會責任下，積極投入科技創新，促\\n進金融服務升級。  \\n以下就AI之影響及國際組織或 主要國家對運用 AI之立\\n場與規定、我國金融業運用 AI現況、訂定 AI原則及政策 之\\n必要性、我國金融業運用 AI之6項核心原則 ，以及本會因\\n應AI發展推動之配套政策等事項進行說明。  \\n \\n \\n1 由於 AI 技術與日俱進，因此各國際組織或各國並未定義 AI，反而聚焦「 AI 系統」，本文交替\\n使用 AI與AI系統。依據經濟合作暨發展組織 (OECD)對「AI 系統」之定義，係指一種以機器為\\n基礎的系統，在給予設定之一組目的下，能透過產製輸出品 (例如預測、建議或決策 )來影響環境。\\n它使用以機器或人類為基礎的數據與輸入品來 (1)感知真實或虛擬環境； (2)萃取這些感知，並透\\n過自動分析 (例如，使用機器學習 )或人工分析，再轉化為模型；以及 (3)使用模型推理來形成結果\\n的選項。 AI系統係設計以不同的自主程度來運作。  ', metadata={'source': './ai.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 載入資料並建立 DataLoader\n",
        "# ------------------------------------\n",
        "train_data, test_data, test_labels = load_and_prepare_data(\n",
        "    csv_file='transaction_data.csv', # 假設你的 CSV 檔案名為 transaction_data.csv\n",
        "    feature_cols=['V1', 'V2'],      # 使用 V1 和 V2 欄位作為特徵\n",
        "    test_size=100                    # 使用最後 100 rows 作為測試集\n",
        ")\n",
        "\n",
        "# 建立訓練集 DataLoader\n",
        "train_dataset = torch.utils.data.TensorDataset(train_data, train_data) # Autoencoder 的輸入和輸出都是相同的資料 (重建)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 建立測試集 DataLoader (注意: 測試集不需要 shuffle)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_data, test_data)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8EvQO0H-Flj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 訓練模型 (Fine-tuning)\n",
        "# ------------------------------------\n",
        "history = {'train_loss': []} # 紀錄訓練過程中的 loss\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # 設定模型為訓練模式\n",
        "    train_loss = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "        inputs = inputs.unsqueeze(1).to(device) # 將輸入資料增加一個 sequence length 維度 (batch_size, seq_len=1, feature_size), 並移動到 GPU (如果有的話)\n",
        "        targets = targets.unsqueeze(1).to(device) # 同樣處理 targets\n",
        "\n",
        "        optimizer.zero_grad() # 清空梯度\n",
        "        outputs = model(inputs) # 前向傳播\n",
        "        loss = criterion(outputs, targets) # 計算 loss (重建誤差)\n",
        "        loss.backward()         # 反向傳播\n",
        "        optimizer.step()        # 更新模型參數\n",
        "\n",
        "        train_loss += loss.item() # 累加 batch loss\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader) # 計算平均 train loss\n",
        "    history['train_loss'].append(avg_train_loss)      # 紀錄平均 train loss\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l8-OeZkn-o0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 異常偵測和評估 (使用測試集)\n",
        "# ------------------------------------\n",
        "model.eval() # 設定模型為評估模式 (關閉 dropout 等)\n",
        "reconstruction_errors = [] # 儲存測試集樣本的重建誤差\n",
        "predicted_anomalies = [] # 儲存預測的異常標籤\n",
        "true_labels_list = []   # 儲存真實標籤 (如果有的話)\n",
        "\n",
        "with torch.no_grad(): # 在評估階段，不需要計算梯度\n",
        "    for inputs, targets in test_dataloader:\n",
        "        inputs = inputs.unsqueeze(1).to(device) # 處理輸入資料，增加 sequence length 維度並移動到 GPU\n",
        "        targets = targets.unsqueeze(1).to(device)\n",
        "        outputs = model(inputs) # 前向傳播，取得重建後的輸出\n",
        "        loss = criterion(outputs, targets) # 計算重建誤差 (MSE)\n",
        "        reconstruction_errors.extend(loss.cpu().numpy()) # 紀錄 batch 的平均重建誤差\n",
        "\n",
        "        # 簡單的異常判斷: 將重建誤差與閾值比較\n",
        "        threshold = np.percentile(reconstruction_errors, 85) # 使用重建誤差的 85 百分位數作為閾值 (可調整)\n",
        "        batch_predicted_anomalies = (loss.cpu().numpy() > threshold).astype(int) # 大於閾值判斷為異常 (1), 否則正常 (0)\n",
        "        predicted_anomalies.extend(batch_predicted_anomalies)\n",
        "\n",
        "        if test_labels is not None:\n",
        "            batch_true_labels = test_labels[len(true_labels_list): len(true_labels_list) + len(inputs)].numpy() # 取得當前 batch 的真實標籤\n",
        "            true_labels_list.extend(batch_true_labels) # 紀錄真實標籤\n",
        "\n",
        "\n",
        "# 將 reconstruction errors 轉換為每個樣本的誤差 (這裡簡化為 batch 平均誤差，實際應用中建議計算每個樣本的誤差)\n",
        "# 由於我們是計算 batch 的平均 MSE Loss, 這裡簡化處理，實際應用中建議調整 dataloader 每次只輸出 batch_size=1 來計算每個樣本的誤差\n",
        "reconstruction_errors_samples = np.repeat(np.array(reconstruction_errors), batch_size)[:len(test_data)] # 粗略將 batch error 擴展到樣本 (需調整)\n",
        "predicted_anomalies_samples = np.repeat(np.array(predicted_anomalies), batch_size)[:len(test_data)] # 粗略將 batch anomaly label 擴展到樣本\n",
        "\n",
        "\n",
        "# 評估模型 (如果測試集有真實標籤)\n",
        "if test_labels is not None:\n",
        "    print(\"\\n--- 評估結果 (基於測試集真實標籤) ---\")\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(true_labels_list[:len(predicted_anomalies_samples)], predicted_anomalies_samples)) # 注意 label 長度對齊\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(true_labels_list[:len(predicted_anomalies_samples)], predicted_anomalies_samples))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJ2WsXVOO_Aw",
        "outputId": "5e9f55e8-ff9d-4c66-ac1e-68f2da099438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name amu/tao-8k. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "), model_name='amu/tao-8k', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, multi_process=False, show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 繪製訓練 Loss 曲線 和 異常分數分佈 (可選)\n",
        "# ------------------------------------\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(reconstruction_errors_samples, bins=50)\n",
        "plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.2f})')\n",
        "plt.title('Test Reconstruction Error Distribution')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- 程式碼執行完畢 ---\")\n",
        "print(\"請查看訓練 Loss 曲線、測試集重建誤差分佈、以及評估結果 (如果測試集有真實標籤)。\")\n",
        "print(\"異常判斷閾值設定為重建誤差的 85 百分位數，您可以調整這個閾值來改變異常偵測的靈敏度。\")"
      ],
      "metadata": {
        "id": "KNNdYOIq_Nip",
        "outputId": "b6faa34f-eab9-4aec-97d0-65cbccfd5605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "共 476 頁\n",
            "最後一頁內容:  page_content='當之教育及培訓，使員工能適應 AI帶來之變革，並盡\\n可能維護其 工作權益。' metadata={'source': './ai.pdf', 'page': 25}\n",
            "全部內容:  現象，保護自然環境，從而促進包容性成長、永續發\n",
            "展及社會福祉。 (二)金融機構在 AI系統運用過程中， 宜對一般員工 提供適 當之教育及培訓，使員工能適應 AI帶來之變革，並盡\n",
            "可能維護其 工作權益。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yvp1br55VJnM",
        "outputId": "4c7ef7cb-260b-4387-92a8-a47b9f4a58e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " '。 (二)金融機構 使用 AI與消費者直接互動 時，應適當揭露 。 \\n六、促進永續發展 (一)金融機構在運用 AI系統時，應確保其發展策略及執行 與永續發展之原則相結合 ，包括減少經濟、社會等不平 等現象，保護自然環境，從而促進包容性成長、永續發\\n展及社會福祉。 (二)金融機構在 AI系統運用過程中， 宜對一般員工 提供適 當之教育及培訓，使員工能適應 AI帶來之變革，並盡\\n可能維護其 工作權益。')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(docs, embeddings, persist_directory=\"db\")\n",
        "db"
      ],
      "metadata": {
        "id": "bhnIMNF0_NlI",
        "outputId": "99342e03-08b3-4bc3-f18a-093d66a30a86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6min 29s, sys: 5min 7s, total: 11min 36s\n",
            "Wall time: 11min 49s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7bccd6c3fd90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.vectorstores import FAISS\n",
        "# results = db.similarity_search(\"\", k=3)\n",
        "# for i in range(len(results)):\n",
        "#   print(results[i])\n",
        "#   print('='*10)\n",
        "# # results[-1]\n",
        "\n",
        "# retriever = db.as_retriever(search_type=\"mmr\")\n",
        "# docs = retriever.get_relevant_documents(\"what about SupTech\")\n",
        "# docs"
      ],
      "metadata": {
        "id": "6bu2-_Uj-o5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "template = \"\"\"\n",
        "role: 你是台灣的金融監督管理委員會之「窗口」，並且說著正體中文\n",
        "question: {question}\n",
        "answer: 回覆之前請檢視自己的答案，不可以捏照回答。並且詳盡可能回答你的客戶\n",
        "context: {context}\n",
        "temperature=0.0\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,retriever=db.as_retriever(search_kwargs={'k':2}), chain_type_kwargs={\"prompt\": qa_prompt}, return_source_documents=True)\n",
        "result = qa_chain({\"question\": myquery, \"context\": myapp})\n",
        "\n",
        "\n",
        "print(qa_chain)\n",
        "\n",
        "\n",
        "res=llm.invoke(prompt)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "Uy911136d4SJ",
        "outputId": "fe52d982-a057-4382-caa6-06ebb0d10aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'myquery' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0882566bf15d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mqa_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqa_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_source_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmyquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmyapp\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_chain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'myquery' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E59Y6cWbd4VA",
        "outputId": "c169788d-c9bc-4940-86d8-ecf4246f7321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**您好！我是台灣金融監督管理委員會的「窗口」，我專門為您解答有關監理科技 (SupTech) 的相關問題。**\n",
            "\n",
            "**SuperTech 的定義是什麼？**\n",
            "\n",
            "SuperTech 是指利用數碼技術和網路技術來建立和營運金融產品和服務的領域。\n",
            "\n",
            "**SuperTech 的監理範圍是什麼？**\n",
            "\n",
            "SuperTech 的監理範圍涵蓋但不限於：\n",
            "\n",
            "* 產品設計與開發\n",
            "* 交易處理\n",
            "* 投資管理\n",
            "* 客戶服務\n",
            "* 安全與合規性\n",
            "\n",
            "**SuperTech 的監理義務是什麼？**\n",
            "\n",
            "SuperTech 的監理義務包括：\n",
            "\n",
            "* 定期監控產品和服務的設計與開發過程\n",
            "* 定期監控交易處理的過程\n",
            "* 定期監控投資管理的過程\n",
            "* 定期監控客戶服務的過程\n",
            "* 定期監控安全與合規性的狀況\n",
            "* 採取適當的措施來確保產品和服務安全與合規\n",
            "\n",
            "**如何遵守 SuperTech 的監理義務？**\n",
            "\n",
            "SuperTech 的監理義務可以透過以下方式實現：\n",
            "\n",
            "* 確保產品和服務的設計與開發過程符合安全與合規標準\n",
            "* 建立嚴格的交易處理流程\n",
            "* 建立完善的投資管理制度\n",
            "* 建立嚴格的客戶服務系統\n",
            "* 建立完善的安全與合規性管理系統\n",
            "\n",
            "**如何與 SuperTech 的監理相關聯繫？**\n",
            "\n",
            "您可以透過以下方式與 SuperTech 的監理相關聯繫：\n",
            "\n",
            "* 金管會網站上的資訊中心\n",
            "* 金管會辦理的監理報告\n",
            "* 金管會辦理的公開會議\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZ-0kH0fd4XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjy8zQacd4Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v82g7Zp8-I-m"
      },
      "source": [
        "## Test Library Setup\n",
        "\n",
        "Next, let's create our test \"library.\"\n",
        "\n",
        "For simplicity's sake, let's say that our \"library\" is simply a **nested directory of `.epub` files**. We can easily see this solution generalizing to, say, a Calibre library with a `metadata.db` database file. We'll leave that extension as an exercise for the reader. 😇\n",
        "\n",
        "Let's pull two `.epub` files from [Project Gutenberg](https://www.gutenberg.org/) for our library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie45lzO0-I-n"
      },
      "outputs": [],
      "source": [
        "!mkdir -p \".test/library/jane-austen\"\n",
        "!mkdir -p \".test/library/victor-hugo\"\n",
        "!wget https://www.gutenberg.org/ebooks/1342.epub.noimages -O \".test/library/jane-austen/pride-and-prejudice.epub\"\n",
        "!wget https://www.gutenberg.org/ebooks/135.epub.noimages -O \".test/library/victor-hugo/les-miserables.epub\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYp-HAmN-I-n"
      },
      "source": [
        "## RAG with LlamaIndex\n",
        "\n",
        "RAG with LlamaIndex, at its core, consists of the following broad phases:\n",
        "\n",
        "1. **Loading**, in which you tell LlamaIndex where your data lives and how to\n",
        "   load it;\n",
        "2. **Indexing**, in which you augment your loaded data to facilitate querying, e.g. with vector embeddings;\n",
        "3. **Querying**, in which you configure an LLM to act as the query interface for\n",
        "   your indexed data.\n",
        "\n",
        "This explanation only scratches at the surface of what's possible with\n",
        "LlamaIndex. For more in-depth details, I highly recommend reading the\n",
        "[\"High-Level Concepts\" page of the LlamaIndex\n",
        "documentation](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8QV16yJ-I-o"
      },
      "source": [
        "### Loading\n",
        "\n",
        "Naturally, let's start with the **loading** phase.\n",
        "\n",
        "I mentioned before that LlamaIndex is designed specifically for RAG. This\n",
        "immediately becomes obvious from its\n",
        "[`SimpleDirectoryReader`](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader.html)\n",
        "construct, which ✨ **magically** ✨ supports a whole host of multi-model file\n",
        "types for free. Conveniently for us, `.epub` is in the supported set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "url8lH9n-I-o"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "loader = SimpleDirectoryReader(\n",
        "    input_dir=\"./.test/\",\n",
        "    recursive=True,\n",
        "    required_exts=[\".epub\"],\n",
        ")\n",
        "\n",
        "documents = loader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWiY0D4-I-p"
      },
      "source": [
        "`SimpleDirectoryReader.load_data()` converts our ebooks into a set of [`Document`s](https://docs.llamaindex.ai/en/stable/api/llama_index.core.schema.Document.html) for LlamaIndex to work with.\n",
        "\n",
        "One important thing to note here is that the documents **have not been chunked at this stage** -- that will happen during indexing. Read on..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mDtpTdn-I-p"
      },
      "source": [
        "### Indexing\n",
        "\n",
        "Next up after **loading** the data is to **index** it. This will allow our RAG pipeline to look up the relevant context for our query to pass to our LLM to **augment** their generated response. This is also where document chunking will take place.\n",
        "\n",
        "[`VectorStoreIndex`](https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index.html)\n",
        "is a \"default\" entrypoint for indexing in LlamaIndex. By default,\n",
        "`VectorStoreIndex` uses a simple, in-memory dictionary to store the indices, but\n",
        "LlamaIndex also supports [a wide variety of vector storage\n",
        "solutions](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html)\n",
        "for you to graduate to as you scale.\n",
        "\n",
        "<Tip>\n",
        "By default, LlamaIndex uses a chunk size of 1024 and a chunk overlap of\n",
        "20. For more details, see the [LlamaIndex\n",
        "documentation](https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies.html#chunk-sizes).\n",
        "</Tip>\n",
        "\n",
        "\n",
        "Like mentioned before, we'll use the\n",
        "[`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) to\n",
        "generate our embeddings. By default, [LlamaIndex uses\n",
        "OpenAI](https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html)\n",
        "(specifically `gpt-3.5-turbo`), which we'd like to avoid given our desire for a lightweight, locally-runnable end-to-end solution.\n",
        "\n",
        "Thankfully, LlamaIndex supports retrieving embedding models from Hugging Face through the convenient `HuggingFaceEmbedding` class, so we'll use that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkJoR1xo-I-p"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mwPjFGz-I-p"
      },
      "source": [
        "We'll pass that in to `VectorStoreIndex` as our embedding model to circumvent the OpenAI default behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK5u69zP-I-p"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=embedding_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuEPvBTQ-I-p"
      },
      "source": [
        "### Querying\n",
        "\n",
        "Now for the final piece of the RAG puzzle -- wiring up the query layer.\n",
        "\n",
        "We'll use Llama 2 for the purposes of this recipe, but I encourage readers to play around with different models to see which produces the \"best\" responses here.\n",
        "\n",
        "First let's start up the Ollama server. Unfortunately, there is no support in the [Ollama Python client](https://github.com/ollama/ollama-python) for actually starting and stopping the server itself, so we'll have to pop out of Python land for this.\n",
        "\n",
        "In a separate terminal, run: `ollama serve`. Remember to terminate this after we're done here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sohcJ3cQ-I-q"
      },
      "source": [
        "Now let's hook Llama 2 up to LlamaIndex and use it as the basis of our query engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqlmG5mZ-I-q"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "llama = Ollama(\n",
        "    model=\"llama2\",\n",
        "    request_timeout=40.0,\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llama)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca8Gyur8-I-q"
      },
      "source": [
        "## Final Result\n",
        "\n",
        "With that, our basic RAG librarian is set up and we can start asking questions about our library. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvSrYLOq-I-q",
        "outputId": "415cfcdf-ddc3-4c68-f454-cde83b798a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the context provided, there are two books available:\n",
            "\n",
            "1. \"Pride and Prejudice\" by Jane Austen\n",
            "2. \"Les Misérables\" by Victor Hugo\n",
            "\n",
            "The context used to derive this answer includes:\n",
            "\n",
            "* The file path for each book, which provides information about the location of the book files on the computer.\n",
            "* The titles of the books, which are mentioned in the context as being available for reading.\n",
            "* A list of words associated with each book, such as \"epub\" and \"notebooks\", which provide additional information about the format and storage location of each book.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"What are the titles of all the books available? Show me the context used to derive your answer.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9gYtakU-I-q",
        "outputId": "3a90cb94-0de9-4af7-900a-18dcd9d3cf3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The main character of 'Pride and Prejudice' is Elizabeth Bennet.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"Who is the main character of 'Pride and Prejudice'?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYqlu4AX-I-r"
      },
      "source": [
        "## Conclusion and Future Improvements\n",
        "\n",
        "We've demonstrated how to build a basic RAG-based \"librarian\" that runs entirely locally, even on Apple silicon Macs. In doing so, we've also carried out a \"grand tour\" of LlamaIndex and how it streamlines the process of setting up RAG-based applications.\n",
        "\n",
        "That said, we've really only scratched the surface of what's possible here. Here are some ideas of how to refine and build upon this foundation.\n",
        "\n",
        "### Forcing Citations\n",
        "\n",
        "To guard against the risk of our librarian hallucinating, how might we require that it provide citations for everything that it says?\n",
        "\n",
        "### Using Extended Metadata\n",
        "\n",
        "Ebook library management solutions like [Calibre](https://calibre-ebook.com/) create additional metadata for ebooks in a library. This can provide information such as publisher or edition that might not be readily available in the text of the book itself. How could we extend our RAG pipeline to account for additional sources of information that aren't `.epub` files?\n",
        "\n",
        "### Efficient Indexing\n",
        "\n",
        "If we were to collect everything we built here into a script/executable, the resulting script would re-index our library on each invocation. For our tiny test library of two files, this is \"fine,\" but for any library of non-trivial size this will very quickly become annoying for users. How could we persist the embedding indices and only update them when the contents of the library have meaningfully changed, e.g. new books have been added?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0297f5ef483e4dc68fd7fbec460e3b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2180a0af632a404791a6cd01d9c29d72",
              "IPY_MODEL_b22fe18e7f3d4adb9e9f379ed9ef70c1",
              "IPY_MODEL_ae9752e84e38428b84785f62c2c98f5f"
            ],
            "layout": "IPY_MODEL_7922b1396b4f459b84817f3726770e22"
          }
        },
        "2180a0af632a404791a6cd01d9c29d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_432fbb6cb3a44a129acbfea14f1ca815",
            "placeholder": "​",
            "style": "IPY_MODEL_3bd8b62f3431423ea9bd07503f73cfec",
            "value": "gemma-2b-it-q8_0.gguf: 100%"
          }
        },
        "b22fe18e7f3d4adb9e9f379ed9ef70c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e00e20e53af34d9285283d143168499c",
            "max": 2669351840,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26b2b02d5a794d6292ea9e76a269f804",
            "value": 2669351840
          }
        },
        "ae9752e84e38428b84785f62c2c98f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83aec98c57594530ac2d9e069fb5c813",
            "placeholder": "​",
            "style": "IPY_MODEL_eca48f6113c640cba713700571c05c55",
            "value": " 2.67G/2.67G [00:23&lt;00:00, 112MB/s]"
          }
        },
        "7922b1396b4f459b84817f3726770e22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "432fbb6cb3a44a129acbfea14f1ca815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bd8b62f3431423ea9bd07503f73cfec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e00e20e53af34d9285283d143168499c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b2b02d5a794d6292ea9e76a269f804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83aec98c57594530ac2d9e069fb5c813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eca48f6113c640cba713700571c05c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}