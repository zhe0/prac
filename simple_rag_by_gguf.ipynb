{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhe0/prac/blob/main/simple_rag_by_gguf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "another example\n",
        "https://drive.google.com/file/d/1L_jceFtHLRBm__xgv_bX2O5iA53FWqDr/view?usp=drive_link"
      ],
      "metadata": {
        "id": "NZaRbtruYN-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%gdown 1L_jceFtHLRBm__xgv_bX2O5iA53FWqDr"
      ],
      "metadata": {
        "id": "U9LjapWbYMM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rag chatbot example\n",
        "* åå°çš„èªè¨€æ¨¡çµ„\n",
        "* æ¸›å°‘å¹»è¦ºç¨‹åº¦"
      ],
      "metadata": {
        "id": "ZJbsrUlcYUAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qqq langchain llama-cpp-python pypdf sentence-transformers chromadb langchain_experimental"
      ],
      "metadata": {
        "id": "QGye1ERE-e5m",
        "outputId": "c23afc14-144a-4531-ab8d-0d393b4f360f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.9/36.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F53QV6f6-I-l",
        "outputId": "539ba866-d60c-4ac1-a75b-5715f1a0e3c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0297f5ef483e4dc68fd7fbec460e3b31",
            "2180a0af632a404791a6cd01d9c29d72",
            "b22fe18e7f3d4adb9e9f379ed9ef70c1",
            "ae9752e84e38428b84785f62c2c98f5f",
            "7922b1396b4f459b84817f3726770e22",
            "432fbb6cb3a44a129acbfea14f1ca815",
            "3bd8b62f3431423ea9bd07503f73cfec",
            "e00e20e53af34d9285283d143168499c",
            "26b2b02d5a794d6292ea9e76a269f804",
            "83aec98c57594530ac2d9e069fb5c813",
            "eca48f6113c640cba713700571c05c55"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gemma-2b-it-q8_0.gguf:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0297f5ef483e4dc68fd7fbec460e3b31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 164 tensors from /root/.cache/huggingface/hub/models--lmstudio-ai--gemma-2b-it-GGUF/snapshots/a0b140bfb922a743f89dd0682a24a17516071ab9/gemma-2b-it-q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
            "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
            "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
            "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
            "llama_model_loader: - type  f32:   37 tensors\n",
            "llama_model_loader: - type q8_0:  127 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256128\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 8\n",
            "llm_load_print_meta: n_head_kv        = 1\n",
            "llm_load_print_meta: n_layer          = 18\n",
            "llm_load_print_meta: n_rot            = 256\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 16384\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 2B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 2.51 B\n",
            "llm_load_print_meta: model size       = 2.48 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = gemma-2b-it\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
            "llm_load_tensors:        CPU buffer size =  2539.93 MiB\n",
            ".............................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
            "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =     0.08 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     7.88 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.file_type': '7', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
            "Using fallback chat format: None\n",
            "\n",
            "llama_print_timings:        load time =    1860.65 ms\n",
            "llama_print_timings:      sample time =     274.71 ms /    54 runs   (    5.09 ms per token,   196.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11709.09 ms /    47 tokens (  249.13 ms per token,     4.01 tokens per second)\n",
            "llama_print_timings:        eval time =   24820.46 ms /    53 runs   (  468.31 ms per token,     2.14 tokens per second)\n",
            "llama_print_timings:       total time =   38822.58 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sure, here is the introduction and translation of the question and answer:\\n\\n**Question:** ä½ æœ‰æ²’æœ‰è½éã€ŒFUBONã€åœ¨å°ç£çš„èªªæ³•å—ï¼Ÿ\\n\\n**Answer:** æˆ‘æ²’æœ‰è½éã€ŒFUBONã€åœ¨å°ç£çš„èªªæ³•ã€‚'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from huggingface_hub import hf_hub_download\n",
        "model_name='lmstudio-ai/gemma-2b-it-GGUF'\n",
        "#model_basename = \"gemma-2b-it-q8_0.gguf\"\n",
        "model_path = hf_hub_download(repo_id=model_name)\n",
        "llm = LlamaCpp(\n",
        "model_path=model_path,\n",
        "temperature=0.0,\n",
        "max_tokens=2000,\n",
        "top_p=.99,\n",
        "verbose=True, # Verbose is required to pass to the callback manager\n",
        ")\n",
        "\n",
        "prompt = \"\"\"\n",
        "Prerequisite: you are in Taiwan and speak their language\n",
        "Question: have you heard 'FUBON' in Taiwan before?\n",
        "Answer: help me introduce 'FUBON' and translate your answer into traditional Chinese\n",
        "\"\"\"\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests,glob\n",
        "\n",
        "url = 'https://www.fsc.gov.tw/uploaddowndoc?file=news/202310171518250.pdf&filedisplay=%E9%99%84%E4%BB%B6_%E9%87%91%E8%9E%8D%E6%A5%AD%E9%81%8B%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%28AI%29%E4%B9%8B%E6%A0%B8%E5%BF%83%E5%8E%9F%E5%89%87%E8%88%87%E7%9B%B8%E9%97%9C%E6%8E%A8%E5%8B%95%E6%94%BF%E7%AD%96.pdf&flag=doc'\n",
        "res=requests.get(url)\n",
        "pdf_path = './ai.pdf'\n",
        "with open(pdf_path,'wb') as file:\n",
        "  file.write(res.content)\n",
        "\n",
        "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
        "pdfs = glob.glob(pdf_path)\n",
        "\n",
        "pi = 0\n",
        "loader = PyPDFLoader(pdfs[pi], )\n",
        "pages = loader.load()\n",
        "pages[0]"
      ],
      "metadata": {
        "id": "fL2n0ve5AA9q",
        "outputId": "ce2bfab9-5091-4a0c-b4d2-f82153f62fe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='1 \\n é‡‘èæ¥­é‹ç”¨ äººå·¥æ™ºæ…§ (AI)ä¹‹æ ¸å¿ƒåŸå‰‡èˆ‡ç›¸é—œæ¨å‹• æ”¿ç­–  \\nä¸€ã€å‰è¨€  \\nè¿‘ä¾† AI1åœ¨é‡‘èæœå‹™ é ˜åŸŸçš„æ‡‰ç”¨æ—¥ç›Šå¢åŠ ï¼Œ ç‚ºé‡‘èç”¢æ¥­\\næä¾›å®¢æˆ¶æœå‹™ å¸¶ä¾†æ•ˆç›Šï¼Œäº¦åŒæ™‚è¡ç”Ÿä¸€äº›æ–°çš„é¢¨éšªå•é¡Œ åŠç›£\\nç†æŒ‘æˆ°ã€‚ ç‚ºå”åŠ©é‡‘èæ©Ÿæ§‹å–„ç”¨ AIç§‘æŠ€å„ªå‹¢ï¼Œä¸¦ èƒ½æœ‰æ•ˆç®¡ç†\\né¢¨éšªã€ç¢ºä¿å…¬å¹³ã€ä¿è­·æ¶ˆè²»è€…æ¬Šç›Šã€ç¶­è­·ç³»çµ±å®‰å…¨åŠå¯¦ç¾æ°¸\\nçºŒç™¼å±•ï¼Œæœ¬æœƒ ä¾æ“šè¡Œæ”¿é™¢ã€Œ è‡ºç£ AIè¡Œå‹•è¨ˆç•« 2.0ã€æ”¿ç­–è¦åŠƒ\\nåŠã€Œæ•¸ä½æ”¿ç­–æ³•åˆ¶å”èª¿ å°ˆæ¡ˆæœƒè­°ã€ä¹‹æ¨å‹•ç­–ç•¥ ï¼Œä¸¦åƒè€ƒå…¨çƒ\\nä¸»è¦åœ‹å®¶ç›£ç†æ©Ÿé—œåŠåœ‹éš›çµ„ç¹”ä¹‹ç›¸é—œæŒ‡å°åŸå‰‡ï¼Œ åŠçµåˆæˆ‘åœ‹\\né‡‘èå¸‚å ´ç™¼å±•ç‹€æ³åŠæœ¬æœƒç›£ç†æ”¿ç­–æ–¹å‘ï¼Œæ“¬å®šé©åˆæˆ‘åœ‹é‡‘è\\næ¥­çš„ 6é …AIæ‡‰ç”¨æ ¸å¿ƒåŸå‰‡ï¼Œä»¥ æœŸå¼•å°é‡‘èæ¥­åœ¨å…¼é¡§æ¶ˆè²»è€…\\næ¬Šç›Šã€é‡‘èå¸‚å ´ç§©åºåŠç¤¾æœƒè²¬ä»»ä¸‹ï¼Œç©æ¥µæŠ•å…¥ç§‘æŠ€å‰µæ–°ï¼Œä¿ƒ\\né€²é‡‘èæœå‹™å‡ç´šã€‚  \\nä»¥ä¸‹å°±AIä¹‹å½±éŸ¿åŠåœ‹éš›çµ„ç¹”æˆ– ä¸»è¦åœ‹å®¶å°é‹ç”¨ AIä¹‹ç«‹\\nå ´èˆ‡è¦å®šã€æˆ‘åœ‹é‡‘èæ¥­é‹ç”¨ AIç¾æ³ã€è¨‚å®š AIåŸå‰‡åŠæ”¿ç­– ä¹‹\\nå¿…è¦æ€§ã€æˆ‘åœ‹é‡‘èæ¥­é‹ç”¨ AIä¹‹6é …æ ¸å¿ƒåŸå‰‡ ï¼Œä»¥åŠæœ¬æœƒå› \\næ‡‰AIç™¼å±•æ¨å‹•ä¹‹é…å¥—æ”¿ç­–ç­‰äº‹é …é€²è¡Œèªªæ˜ã€‚  \\n \\n \\n1 ç”±æ–¼ AI æŠ€è¡“èˆ‡æ—¥ä¿±é€²ï¼Œå› æ­¤å„åœ‹éš›çµ„ç¹”æˆ–å„åœ‹ä¸¦æœªå®šç¾© AIï¼Œåè€Œèšç„¦ã€Œ AI ç³»çµ±ã€ï¼Œæœ¬æ–‡äº¤æ›¿\\nä½¿ç”¨ AIèˆ‡AIç³»çµ±ã€‚ä¾æ“šç¶“æ¿Ÿåˆä½œæš¨ç™¼å±•çµ„ç¹” (OECD)å°ã€ŒAI ç³»çµ±ã€ä¹‹å®šç¾©ï¼Œä¿‚æŒ‡ä¸€ç¨®ä»¥æ©Ÿå™¨ç‚º\\nåŸºç¤çš„ç³»çµ±ï¼Œåœ¨çµ¦äºˆè¨­å®šä¹‹ä¸€çµ„ç›®çš„ä¸‹ï¼Œèƒ½é€éç”¢è£½è¼¸å‡ºå“ (ä¾‹å¦‚é æ¸¬ã€å»ºè­°æˆ–æ±ºç­– )ä¾†å½±éŸ¿ç’°å¢ƒã€‚\\nå®ƒä½¿ç”¨ä»¥æ©Ÿå™¨æˆ–äººé¡ç‚ºåŸºç¤çš„æ•¸æ“šèˆ‡è¼¸å…¥å“ä¾† (1)æ„ŸçŸ¥çœŸå¯¦æˆ–è™›æ“¬ç’°å¢ƒï¼› (2)èƒå–é€™äº›æ„ŸçŸ¥ï¼Œä¸¦é€\\néè‡ªå‹•åˆ†æ (ä¾‹å¦‚ï¼Œä½¿ç”¨æ©Ÿå™¨å­¸ç¿’ )æˆ–äººå·¥åˆ†æï¼Œå†è½‰åŒ–ç‚ºæ¨¡å‹ï¼›ä»¥åŠ (3)ä½¿ç”¨æ¨¡å‹æ¨ç†ä¾†å½¢æˆçµæœ\\nçš„é¸é …ã€‚ AIç³»çµ±ä¿‚è¨­è¨ˆä»¥ä¸åŒçš„è‡ªä¸»ç¨‹åº¦ä¾†é‹ä½œã€‚  ', metadata={'source': './ai.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install --upgrade --quiet  nest_asyncio\n",
        "\n",
        "# # fixes a bug with asyncio and jupyter\n",
        "# import nest_asyncio\n",
        "\n",
        "# nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "8EvQO0H-Flj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # from langchain_community.document_loaders import WebBaseLoader\n",
        "# # loader = WebBaseLoader([\"https://www.fubon.com/life/product/personal/life-refund/IBZ\", \"https://www.fubon.com/life/product/personal/life-refund/PAI\"])\n",
        "# # loader = WebBaseLoader(\"https://www.fubon.com/life/direct/?utm_source=fubonlife&utm_medium=menu\")\n",
        "\n",
        "\n",
        "# from langchain_community.document_loaders import AsyncHtmlLoader\n",
        "\n",
        "# urls = [\"https://www.espn.com\", \"https://www.fubon.com/life/direct/?utm_source=fubonlife&utm_medium=menu\"]\n",
        "# loader = AsyncHtmlLoader(urls)\n",
        "# docs = loader.load()\n",
        "# docs"
      ],
      "metadata": {
        "id": "l8-OeZkn-o0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "# model_name=\"aspire/acge_text_embedding\"\n",
        "model_name='amu/tao-8k'\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# https://huggingface.co/spaces/mteb/leaderboard #aspire/acge_text_embedding#amu/tao-8k\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={\"device\": device},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "embeddings"
      ],
      "metadata": {
        "id": "DJ2WsXVOO_Aw",
        "outputId": "5e9f55e8-ff9d-4c66-ac1e-68f2da099438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name amu/tao-8k. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "), model_name='amu/tao-8k', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, multi_process=False, show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "chunk_size=50\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=int(chunk_size/10),\n",
        "    length_function=len,\n",
        "    is_separator_regex=True)\n",
        "texts = text_splitter.split_documents(pages)\n",
        "print('å…±',len(texts),'é ')\n",
        "print('æœ€å¾Œä¸€é å…§å®¹: ',texts[-1])\n",
        "\n",
        "all_texts=' '.join([i.page_content for i in texts])\n",
        "print('å…¨éƒ¨å…§å®¹: ',all_texts[-100:])"
      ],
      "metadata": {
        "id": "KNNdYOIq_Nip",
        "outputId": "b6faa34f-eab9-4aec-97d0-65cbccfd5605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å…± 476 é \n",
            "æœ€å¾Œä¸€é å…§å®¹:  page_content='ç•¶ä¹‹æ•™è‚²åŠåŸ¹è¨“ï¼Œä½¿å“¡å·¥èƒ½é©æ‡‰ AIå¸¶ä¾†ä¹‹è®Šé©ï¼Œä¸¦ç›¡\\nå¯èƒ½ç¶­è­·å…¶ å·¥ä½œæ¬Šç›Šã€‚' metadata={'source': './ai.pdf', 'page': 25}\n",
            "å…¨éƒ¨å…§å®¹:  ç¾è±¡ï¼Œä¿è­·è‡ªç„¶ç’°å¢ƒï¼Œå¾è€Œä¿ƒé€²åŒ…å®¹æ€§æˆé•·ã€æ°¸çºŒç™¼\n",
            "å±•åŠç¤¾æœƒç¦ç¥‰ã€‚ (äºŒ)é‡‘èæ©Ÿæ§‹åœ¨ AIç³»çµ±é‹ç”¨éç¨‹ä¸­ï¼Œ å®œå°ä¸€èˆ¬å“¡å·¥ æä¾›é© ç•¶ä¹‹æ•™è‚²åŠåŸ¹è¨“ï¼Œä½¿å“¡å·¥èƒ½é©æ‡‰ AIå¸¶ä¾†ä¹‹è®Šé©ï¼Œä¸¦ç›¡\n",
            "å¯èƒ½ç¶­è­·å…¶ å·¥ä½œæ¬Šç›Šã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = SemanticChunker(embeddings)\n",
        "docs = text_splitter.create_documents([all_texts])\n",
        "len(docs),docs[0].page_content[-200:]"
      ],
      "metadata": {
        "id": "Yvp1br55VJnM",
        "outputId": "4c7ef7cb-260b-4387-92a8-a47b9f4a58e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'ã€‚ (äºŒ)é‡‘èæ©Ÿæ§‹ ä½¿ç”¨ AIèˆ‡æ¶ˆè²»è€…ç›´æ¥äº’å‹• æ™‚ï¼Œæ‡‰é©ç•¶æ­éœ² ã€‚ \\nå…­ã€ä¿ƒé€²æ°¸çºŒç™¼å±• (ä¸€)é‡‘èæ©Ÿæ§‹åœ¨é‹ç”¨ AIç³»çµ±æ™‚ï¼Œæ‡‰ç¢ºä¿å…¶ç™¼å±•ç­–ç•¥åŠåŸ·è¡Œ èˆ‡æ°¸çºŒç™¼å±•ä¹‹åŸå‰‡ç›¸çµåˆ ï¼ŒåŒ…æ‹¬æ¸›å°‘ç¶“æ¿Ÿã€ç¤¾æœƒç­‰ä¸å¹³ ç­‰ç¾è±¡ï¼Œä¿è­·è‡ªç„¶ç’°å¢ƒï¼Œå¾è€Œä¿ƒé€²åŒ…å®¹æ€§æˆé•·ã€æ°¸çºŒç™¼\\nå±•åŠç¤¾æœƒç¦ç¥‰ã€‚ (äºŒ)é‡‘èæ©Ÿæ§‹åœ¨ AIç³»çµ±é‹ç”¨éç¨‹ä¸­ï¼Œ å®œå°ä¸€èˆ¬å“¡å·¥ æä¾›é© ç•¶ä¹‹æ•™è‚²åŠåŸ¹è¨“ï¼Œä½¿å“¡å·¥èƒ½é©æ‡‰ AIå¸¶ä¾†ä¹‹è®Šé©ï¼Œä¸¦ç›¡\\nå¯èƒ½ç¶­è­·å…¶ å·¥ä½œæ¬Šç›Šã€‚')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(docs, embeddings, persist_directory=\"db\")\n",
        "db"
      ],
      "metadata": {
        "id": "bhnIMNF0_NlI",
        "outputId": "99342e03-08b3-4bc3-f18a-093d66a30a86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6min 29s, sys: 5min 7s, total: 11min 36s\n",
            "Wall time: 11min 49s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7bccd6c3fd90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.vectorstores import FAISS\n",
        "# results = db.similarity_search(\"\", k=3)\n",
        "# for i in range(len(results)):\n",
        "#   print(results[i])\n",
        "#   print('='*10)\n",
        "# # results[-1]\n",
        "\n",
        "# retriever = db.as_retriever(search_type=\"mmr\")\n",
        "# docs = retriever.get_relevant_documents(\"what about SupTech\")\n",
        "# docs"
      ],
      "metadata": {
        "id": "6bu2-_Uj-o5Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "template = \"\"\"\n",
        "role: ä½ æ˜¯å°ç£çš„é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒä¹‹ã€Œçª—å£ã€ï¼Œä¸¦ä¸”èªªè‘—æ­£é«”ä¸­æ–‡\n",
        "question: {question}\n",
        "answer: å›è¦†ä¹‹å‰è«‹æª¢è¦–è‡ªå·±çš„ç­”æ¡ˆï¼Œä¸å¯ä»¥æç…§å›ç­”ã€‚ä¸¦ä¸”è©³ç›¡å¯èƒ½å›ç­”ä½ çš„å®¢æˆ¶\n",
        "context: {context}\n",
        "temperature=0.0\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,retriever=db.as_retriever(search_kwargs={'k':2}), chain_type_kwargs={\"prompt\": qa_prompt}, return_source_documents=True)\n",
        "result = qa_chain({\"question\": myquery, \"context\": myapp})\n",
        "\n",
        "\n",
        "print(qa_chain)\n",
        "\n",
        "\n",
        "res=llm.invoke(prompt)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "Uy911136d4SJ",
        "outputId": "fe52d982-a057-4382-caa6-06ebb0d10aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'myquery' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0882566bf15d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mqa_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqa_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_source_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmyquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmyapp\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_chain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'myquery' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E59Y6cWbd4VA",
        "outputId": "c169788d-c9bc-4940-86d8-ecf4246f7321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**æ‚¨å¥½ï¼æˆ‘æ˜¯å°ç£é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒçš„ã€Œçª—å£ã€ï¼Œæˆ‘å°ˆé–€ç‚ºæ‚¨è§£ç­”æœ‰é—œç›£ç†ç§‘æŠ€ (SupTech) çš„ç›¸é—œå•é¡Œã€‚**\n",
            "\n",
            "**SuperTech çš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ**\n",
            "\n",
            "SuperTech æ˜¯æŒ‡åˆ©ç”¨æ•¸ç¢¼æŠ€è¡“å’Œç¶²è·¯æŠ€è¡“ä¾†å»ºç«‹å’Œç‡Ÿé‹é‡‘èç”¢å“å’Œæœå‹™çš„é ˜åŸŸã€‚\n",
            "\n",
            "**SuperTech çš„ç›£ç†ç¯„åœæ˜¯ä»€éº¼ï¼Ÿ**\n",
            "\n",
            "SuperTech çš„ç›£ç†ç¯„åœæ¶µè“‹ä½†ä¸é™æ–¼ï¼š\n",
            "\n",
            "* ç”¢å“è¨­è¨ˆèˆ‡é–‹ç™¼\n",
            "* äº¤æ˜“è™•ç†\n",
            "* æŠ•è³‡ç®¡ç†\n",
            "* å®¢æˆ¶æœå‹™\n",
            "* å®‰å…¨èˆ‡åˆè¦æ€§\n",
            "\n",
            "**SuperTech çš„ç›£ç†ç¾©å‹™æ˜¯ä»€éº¼ï¼Ÿ**\n",
            "\n",
            "SuperTech çš„ç›£ç†ç¾©å‹™åŒ…æ‹¬ï¼š\n",
            "\n",
            "* å®šæœŸç›£æ§ç”¢å“å’Œæœå‹™çš„è¨­è¨ˆèˆ‡é–‹ç™¼éç¨‹\n",
            "* å®šæœŸç›£æ§äº¤æ˜“è™•ç†çš„éç¨‹\n",
            "* å®šæœŸç›£æ§æŠ•è³‡ç®¡ç†çš„éç¨‹\n",
            "* å®šæœŸç›£æ§å®¢æˆ¶æœå‹™çš„éç¨‹\n",
            "* å®šæœŸç›£æ§å®‰å…¨èˆ‡åˆè¦æ€§çš„ç‹€æ³\n",
            "* æ¡å–é©ç•¶çš„æªæ–½ä¾†ç¢ºä¿ç”¢å“å’Œæœå‹™å®‰å…¨èˆ‡åˆè¦\n",
            "\n",
            "**å¦‚ä½•éµå®ˆ SuperTech çš„ç›£ç†ç¾©å‹™ï¼Ÿ**\n",
            "\n",
            "SuperTech çš„ç›£ç†ç¾©å‹™å¯ä»¥é€éä»¥ä¸‹æ–¹å¼å¯¦ç¾ï¼š\n",
            "\n",
            "* ç¢ºä¿ç”¢å“å’Œæœå‹™çš„è¨­è¨ˆèˆ‡é–‹ç™¼éç¨‹ç¬¦åˆå®‰å…¨èˆ‡åˆè¦æ¨™æº–\n",
            "* å»ºç«‹åš´æ ¼çš„äº¤æ˜“è™•ç†æµç¨‹\n",
            "* å»ºç«‹å®Œå–„çš„æŠ•è³‡ç®¡ç†åˆ¶åº¦\n",
            "* å»ºç«‹åš´æ ¼çš„å®¢æˆ¶æœå‹™ç³»çµ±\n",
            "* å»ºç«‹å®Œå–„çš„å®‰å…¨èˆ‡åˆè¦æ€§ç®¡ç†ç³»çµ±\n",
            "\n",
            "**å¦‚ä½•èˆ‡ SuperTech çš„ç›£ç†ç›¸é—œè¯ç¹«ï¼Ÿ**\n",
            "\n",
            "æ‚¨å¯ä»¥é€éä»¥ä¸‹æ–¹å¼èˆ‡ SuperTech çš„ç›£ç†ç›¸é—œè¯ç¹«ï¼š\n",
            "\n",
            "* é‡‘ç®¡æœƒç¶²ç«™ä¸Šçš„è³‡è¨Šä¸­å¿ƒ\n",
            "* é‡‘ç®¡æœƒè¾¦ç†çš„ç›£ç†å ±å‘Š\n",
            "* é‡‘ç®¡æœƒè¾¦ç†çš„å…¬é–‹æœƒè­°\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZ-0kH0fd4XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjy8zQacd4Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v82g7Zp8-I-m"
      },
      "source": [
        "## Test Library Setup\n",
        "\n",
        "Next, let's create our test \"library.\"\n",
        "\n",
        "For simplicity's sake, let's say that our \"library\" is simply a **nested directory of `.epub` files**. We can easily see this solution generalizing to, say, a Calibre library with a `metadata.db` database file. We'll leave that extension as an exercise for the reader. ğŸ˜‡\n",
        "\n",
        "Let's pull two `.epub` files from [Project Gutenberg](https://www.gutenberg.org/) for our library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie45lzO0-I-n"
      },
      "outputs": [],
      "source": [
        "!mkdir -p \".test/library/jane-austen\"\n",
        "!mkdir -p \".test/library/victor-hugo\"\n",
        "!wget https://www.gutenberg.org/ebooks/1342.epub.noimages -O \".test/library/jane-austen/pride-and-prejudice.epub\"\n",
        "!wget https://www.gutenberg.org/ebooks/135.epub.noimages -O \".test/library/victor-hugo/les-miserables.epub\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYp-HAmN-I-n"
      },
      "source": [
        "## RAG with LlamaIndex\n",
        "\n",
        "RAG with LlamaIndex, at its core, consists of the following broad phases:\n",
        "\n",
        "1. **Loading**, in which you tell LlamaIndex where your data lives and how to\n",
        "   load it;\n",
        "2. **Indexing**, in which you augment your loaded data to facilitate querying, e.g. with vector embeddings;\n",
        "3. **Querying**, in which you configure an LLM to act as the query interface for\n",
        "   your indexed data.\n",
        "\n",
        "This explanation only scratches at the surface of what's possible with\n",
        "LlamaIndex. For more in-depth details, I highly recommend reading the\n",
        "[\"High-Level Concepts\" page of the LlamaIndex\n",
        "documentation](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8QV16yJ-I-o"
      },
      "source": [
        "### Loading\n",
        "\n",
        "Naturally, let's start with the **loading** phase.\n",
        "\n",
        "I mentioned before that LlamaIndex is designed specifically for RAG. This\n",
        "immediately becomes obvious from its\n",
        "[`SimpleDirectoryReader`](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader.html)\n",
        "construct, which âœ¨ **magically** âœ¨ supports a whole host of multi-model file\n",
        "types for free. Conveniently for us, `.epub` is in the supported set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "url8lH9n-I-o"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "loader = SimpleDirectoryReader(\n",
        "    input_dir=\"./.test/\",\n",
        "    recursive=True,\n",
        "    required_exts=[\".epub\"],\n",
        ")\n",
        "\n",
        "documents = loader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWiY0D4-I-p"
      },
      "source": [
        "`SimpleDirectoryReader.load_data()` converts our ebooks into a set of [`Document`s](https://docs.llamaindex.ai/en/stable/api/llama_index.core.schema.Document.html) for LlamaIndex to work with.\n",
        "\n",
        "One important thing to note here is that the documents **have not been chunked at this stage** -- that will happen during indexing. Read on..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mDtpTdn-I-p"
      },
      "source": [
        "### Indexing\n",
        "\n",
        "Next up after **loading** the data is to **index** it. This will allow our RAG pipeline to look up the relevant context for our query to pass to our LLM to **augment** their generated response. This is also where document chunking will take place.\n",
        "\n",
        "[`VectorStoreIndex`](https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index.html)\n",
        "is a \"default\" entrypoint for indexing in LlamaIndex. By default,\n",
        "`VectorStoreIndex` uses a simple, in-memory dictionary to store the indices, but\n",
        "LlamaIndex also supports [a wide variety of vector storage\n",
        "solutions](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html)\n",
        "for you to graduate to as you scale.\n",
        "\n",
        "<Tip>\n",
        "By default, LlamaIndex uses a chunk size of 1024 and a chunk overlap of\n",
        "20. For more details, see the [LlamaIndex\n",
        "documentation](https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies.html#chunk-sizes).\n",
        "</Tip>\n",
        "\n",
        "\n",
        "Like mentioned before, we'll use the\n",
        "[`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) to\n",
        "generate our embeddings. By default, [LlamaIndex uses\n",
        "OpenAI](https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html)\n",
        "(specifically `gpt-3.5-turbo`), which we'd like to avoid given our desire for a lightweight, locally-runnable end-to-end solution.\n",
        "\n",
        "Thankfully, LlamaIndex supports retrieving embedding models from Hugging Face through the convenient `HuggingFaceEmbedding` class, so we'll use that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkJoR1xo-I-p"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mwPjFGz-I-p"
      },
      "source": [
        "We'll pass that in to `VectorStoreIndex` as our embedding model to circumvent the OpenAI default behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK5u69zP-I-p"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=embedding_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuEPvBTQ-I-p"
      },
      "source": [
        "### Querying\n",
        "\n",
        "Now for the final piece of the RAG puzzle -- wiring up the query layer.\n",
        "\n",
        "We'll use Llama 2 for the purposes of this recipe, but I encourage readers to play around with different models to see which produces the \"best\" responses here.\n",
        "\n",
        "First let's start up the Ollama server. Unfortunately, there is no support in the [Ollama Python client](https://github.com/ollama/ollama-python) for actually starting and stopping the server itself, so we'll have to pop out of Python land for this.\n",
        "\n",
        "In a separate terminal, run: `ollama serve`. Remember to terminate this after we're done here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sohcJ3cQ-I-q"
      },
      "source": [
        "Now let's hook Llama 2 up to LlamaIndex and use it as the basis of our query engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqlmG5mZ-I-q"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "llama = Ollama(\n",
        "    model=\"llama2\",\n",
        "    request_timeout=40.0,\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llama)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca8Gyur8-I-q"
      },
      "source": [
        "## Final Result\n",
        "\n",
        "With that, our basic RAG librarian is set up and we can start asking questions about our library. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvSrYLOq-I-q",
        "outputId": "415cfcdf-ddc3-4c68-f454-cde83b798a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the context provided, there are two books available:\n",
            "\n",
            "1. \"Pride and Prejudice\" by Jane Austen\n",
            "2. \"Les MisÃ©rables\" by Victor Hugo\n",
            "\n",
            "The context used to derive this answer includes:\n",
            "\n",
            "* The file path for each book, which provides information about the location of the book files on the computer.\n",
            "* The titles of the books, which are mentioned in the context as being available for reading.\n",
            "* A list of words associated with each book, such as \"epub\" and \"notebooks\", which provide additional information about the format and storage location of each book.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"What are the titles of all the books available? Show me the context used to derive your answer.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9gYtakU-I-q",
        "outputId": "3a90cb94-0de9-4af7-900a-18dcd9d3cf3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The main character of 'Pride and Prejudice' is Elizabeth Bennet.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"Who is the main character of 'Pride and Prejudice'?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYqlu4AX-I-r"
      },
      "source": [
        "## Conclusion and Future Improvements\n",
        "\n",
        "We've demonstrated how to build a basic RAG-based \"librarian\" that runs entirely locally, even on Apple silicon Macs. In doing so, we've also carried out a \"grand tour\" of LlamaIndex and how it streamlines the process of setting up RAG-based applications.\n",
        "\n",
        "That said, we've really only scratched the surface of what's possible here. Here are some ideas of how to refine and build upon this foundation.\n",
        "\n",
        "### Forcing Citations\n",
        "\n",
        "To guard against the risk of our librarian hallucinating, how might we require that it provide citations for everything that it says?\n",
        "\n",
        "### Using Extended Metadata\n",
        "\n",
        "Ebook library management solutions like [Calibre](https://calibre-ebook.com/) create additional metadata for ebooks in a library. This can provide information such as publisher or edition that might not be readily available in the text of the book itself. How could we extend our RAG pipeline to account for additional sources of information that aren't `.epub` files?\n",
        "\n",
        "### Efficient Indexing\n",
        "\n",
        "If we were to collect everything we built here into a script/executable, the resulting script would re-index our library on each invocation. For our tiny test library of two files, this is \"fine,\" but for any library of non-trivial size this will very quickly become annoying for users. How could we persist the embedding indices and only update them when the contents of the library have meaningfully changed, e.g. new books have been added?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0297f5ef483e4dc68fd7fbec460e3b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2180a0af632a404791a6cd01d9c29d72",
              "IPY_MODEL_b22fe18e7f3d4adb9e9f379ed9ef70c1",
              "IPY_MODEL_ae9752e84e38428b84785f62c2c98f5f"
            ],
            "layout": "IPY_MODEL_7922b1396b4f459b84817f3726770e22"
          }
        },
        "2180a0af632a404791a6cd01d9c29d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_432fbb6cb3a44a129acbfea14f1ca815",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3bd8b62f3431423ea9bd07503f73cfec",
            "value": "gemma-2b-it-q8_0.gguf:â€‡100%"
          }
        },
        "b22fe18e7f3d4adb9e9f379ed9ef70c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e00e20e53af34d9285283d143168499c",
            "max": 2669351840,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26b2b02d5a794d6292ea9e76a269f804",
            "value": 2669351840
          }
        },
        "ae9752e84e38428b84785f62c2c98f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83aec98c57594530ac2d9e069fb5c813",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eca48f6113c640cba713700571c05c55",
            "value": "â€‡2.67G/2.67Gâ€‡[00:23&lt;00:00,â€‡112MB/s]"
          }
        },
        "7922b1396b4f459b84817f3726770e22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "432fbb6cb3a44a129acbfea14f1ca815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bd8b62f3431423ea9bd07503f73cfec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e00e20e53af34d9285283d143168499c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b2b02d5a794d6292ea9e76a269f804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83aec98c57594530ac2d9e069fb5c813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eca48f6113c640cba713700571c05c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}