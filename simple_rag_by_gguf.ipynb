{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhe0/prac/blob/main/simple_rag_by_gguf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rag chatbot example\n",
        "* 偏小的語言模組\n",
        "* 減少幻覺程度"
      ],
      "metadata": {
        "id": "ZJbsrUlcYUAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "!pip install -q kaggle\n",
        "!chmod 600 ~/kaggle.json # 設定權限，確保只有使用者可以讀取\n",
        "!cp kaggle.json /root/.config/kaggle/ # move the key to the folder\n",
        "import kaggle\n",
        "print(\"Kaggle API 憑證設定完成！\")\n",
        "\n",
        "\n",
        "# 3. 下載 Kaggle Dataset\n",
        "# ------------------------------------\n",
        "# **重要:** 你需要知道你要下載的 Kaggle Dataset 的名稱 (通常是 'owner/dataset-name')\n",
        "# 例如，如果要下載 'creditcardfraud' 這個資料集 (示範用，你可以替換成你的資料集)\n",
        "dataset_name = 'mlg-ulb/creditcardfraud' #  <--  請在這裡替換成你要下載的 Kaggle Dataset 名稱\n",
        "\n",
        "output_path = './kaggle_data' # 設定資料集下載的路徑 (Colab notebook 的當前目錄下的 kaggle_data 資料夾)\n",
        "!mkdir -p {output_path} # 建立資料夾 (如果不存在)\n",
        "\n",
        "print(f\"正在下載 Kaggle Dataset: {dataset_name} 到 {output_path}...\")\n",
        "!kaggle datasets download -d {dataset_name} -p {output_path}\n",
        "print(\"Kaggle Dataset 下載完成！\")\n",
        "\n",
        "\n",
        "# 4. 解壓縮下載的資料集 (如果需要)\n",
        "# ------------------------------------\n",
        "# Kaggle Dataset 通常會下載成 zip 檔案，需要解壓縮才能讀取裡面的資料檔案\n",
        "import zipfile\n",
        "\n",
        "zip_file_path = os.path.join(output_path, f'{dataset_name.split(\"/\")[1]}.zip') # 假設下載的 zip 檔名與 dataset name 相關\n",
        "extract_path = output_path # 解壓縮到同一個 output_path 資料夾\n",
        "\n",
        "print(f\"正在解壓縮檔案: {zip_file_path} 到 {extract_path}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"資料集解壓縮完成。\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"找不到 zip 檔案: {zip_file_path}。 如果下載的不是 zip 檔案，請跳過此步驟。\")\n",
        "\n",
        "\n",
        "# 5. 讀取資料到 Pandas DataFrame (假設資料是 CSV 格式)\n",
        "# ------------------------------------\n",
        "csv_file_name = 'creditcard.csv' # <-- 請根據你的資料集修改 CSV 檔案名稱 (如果適用)\n",
        "csv_file_path = os.path.join(extract_path, csv_file_name) # 假設 CSV 檔案在解壓縮後的資料夾中\n",
        "\n",
        "print(f\"正在讀取 CSV 檔案: {csv_file_path}...\")\n",
        "try:\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "    print(\"CSV 檔案讀取成功，Pandas DataFrame 資訊:\")\n",
        "    print(df.head()) # 印出 DataFrame 的前幾行\n",
        "    print(df.info()) # 印出 DataFrame 的資訊 (欄位名稱, 資料型別, 非空值數量等)\n",
        "\n",
        "    # 6. 將 Pandas DataFrame 轉換成 PyTorch Tensor (範例)\n",
        "    # ------------------------------------\n",
        "    # # 假設你要使用 'V1', 'V2', 'Amount' 這些欄位作為特徵\n",
        "    # feature_cols = ['V1', 'V2', 'Amount'] # <-- 請根據你的需求修改特徵欄位\n",
        "    features = df[[i for i in df.columns if not i == 'Class']].values # 取得特徵值 (NumPy array)\n",
        "    features_tensor = torch.tensor(features, dtype=torch.float32) # 轉換成 PyTorch FloatTensor\n",
        "\n",
        "    # 如果有 'Class' 欄位作為標籤\n",
        "    if 'Class' in df.columns:\n",
        "        labels = df['Class'].values # 取得標籤值 (NumPy array)\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.long) # 轉換成 PyTorch LongTensor (整數標籤)\n",
        "        print(\"\\n特徵 Tensor (features_tensor) shape:\", features_tensor.shape)\n",
        "        print(\"標籤 Tensor (labels_tensor) shape:\", labels_tensor.shape)\n",
        "    else:\n",
        "        print(\"\\n特徵 Tensor (features_tensor) shape:\", features_tensor.shape)\n",
        "        print(\"沒有 'Class' 欄位，僅轉換特徵 Tensor。\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- 從 Kaggle Dataset 讀取資料流程完成 ---\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"找不到 CSV 檔案: {csv_file_path}。 請檢查檔案路徑和檔案名稱是否正確。\")\n",
        "except Exception as e:\n",
        "    print(f\"讀取 CSV 檔案時發生錯誤: {e}\")\n"
      ],
      "metadata": {
        "id": "QGye1ERE-e5m",
        "outputId": "4698e812-40bd-4239-d663-68f24da341c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/kaggle.json': No such file or directory\n",
            "Kaggle API 憑證設定完成！\n",
            "正在下載 Kaggle Dataset: mlg-ulb/creditcardfraud 到 ./kaggle_data...\n",
            "Dataset URL: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
            "License(s): DbCL-1.0\n",
            "creditcardfraud.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Kaggle Dataset 下載完成！\n",
            "正在解壓縮檔案: ./kaggle_data/creditcardfraud.zip 到 ./kaggle_data...\n",
            "資料集解壓縮完成。\n",
            "正在讀取 CSV 檔案: ./kaggle_data/creditcard.csv...\n",
            "CSV 檔案讀取成功，Pandas DataFrame 資訊:\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n",
            "\n",
            "特徵 Tensor (features_tensor) shape: torch.Size([284807, 30])\n",
            "標籤 Tensor (labels_tensor) shape: torch.Size([284807])\n",
            "\n",
            "--- 從 Kaggle Dataset 讀取資料流程完成 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self, feature_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding_encoder = nn.Linear(feature_size, d_model) # 輸入特徵 Embedding 層\n",
        "        self.embedding_decoder = nn.Linear(d_model, feature_size) # 輸出特徵 De-embedding 層 (用於重建)\n",
        "\n",
        "        # Encoder 層堆疊\n",
        "        encoder_layers = [TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_encoder_layers)]\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Decoder 層堆疊\n",
        "        decoder_layers = [TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_decoder_layers)]\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Encoder 部分\n",
        "        enc_src = self.embedding_encoder(src) # 將輸入特徵轉換到 d_model 維度 [batch_size, seq_len, d_model]\n",
        "        memory = self.encoder(enc_src)       # 通過 Encoder 層 [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Decoder 部分 - 正確的 Sequential 迭代方式\n",
        "        tgt = memory # 初始化 decoder 的 input 為 encoder 的 memory (對於 Autoencoder 來說，通常將 encoder 的輸出作為 decoder 的初始輸入)\n",
        "        for decoder_layer in self.decoder: # 迭代 self.decoder (nn.Sequential 容器) 中的每一層\n",
        "            tgt = decoder_layer(tgt, memory) # 顯式調用每個 decoder_layer 的 forward 方法，並傳遞 tgt 和 memory\n",
        "\n",
        "        output = self.embedding_decoder(tgt) # [batch_size, seq_len, feature_size]\n",
        "        return output"
      ],
      "metadata": {
        "id": "zkh34SESSg2y"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "F53QV6f6-I-l"
      },
      "outputs": [],
      "source": [
        "# 2. 從 CSV 檔案載入資料並準備訓練/測試集\n",
        "# ------------------------------------\n",
        "def load_and_prepare_data(csv_file='transaction_data.csv', feature_cols=None, test_size=100):\n",
        "    \"\"\"\n",
        "    從 CSV 檔案載入資料，選擇特徵欄位，並分割成訓練集和測試集。\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): CSV 檔案路徑 (假設已上傳到 Colab).\n",
        "        feature_cols (list): 要使用的特徵欄位名稱列表.\n",
        "        test_size (int): 測試集大小 (取最後幾 rows 作為測試集).\n",
        "\n",
        "    Returns:\n",
        "        tuple: 訓練集 (torch.Tensor), 測試集 (torch.Tensor), 真實標籤 (測試集, torch.Tensor, 如果 CSV 包含 'Class' 欄位).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # 選擇特徵欄位\n",
        "    if feature_cols is None:\n",
        "        feature_cols = [i for i in df.columns if not i == 'Class'] # 假設 V1, V2, Amount 都是特徵欄位，或者使用全部\n",
        "    X = df[feature_cols].values\n",
        "    y = None # 預設沒有標籤\n",
        "    if 'Class' in df.columns:\n",
        "        y = df['Class'].values # 如果 CSV 包含 'Class' 欄位，則讀取標籤\n",
        "\n",
        "    # 將資料轉換成 PyTorch tensors\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    if y is not None:\n",
        "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "    else:\n",
        "        y_tensor = None\n",
        "\n",
        "    # 分割訓練集和測試集 (取最後 test_size rows 作為測試集)\n",
        "    train_data = X_tensor[:-test_size]\n",
        "    test_data = X_tensor[-test_size:]\n",
        "    test_labels = None\n",
        "    if y_tensor is not None:\n",
        "        test_labels = y_tensor[-test_size:]\n",
        "\n",
        "    return train_data, test_data, test_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 設定超參數和模型、優化器、損失函數\n",
        "# ------------------------------------\n",
        "feature_size = len(df.columns)-1 # 輸入特徵維度2 (V1, V2)\n",
        "sequence_length = 1 # 每個樣本視為長度為 1 的序列 (因為我們目前是獨立處理每個 row)\n",
        "d_model = 64       # Transformer 模型中的 embedding dimension\n",
        "nhead = 2          # Multi-head attention head 數量\n",
        "num_encoder_layers = 2 # Encoder 層數\n",
        "num_decoder_layers = 2 # Decoder 層數\n",
        "dim_feedforward = 128 # Feedforward network hidden layer dimension\n",
        "dropout = 0.1\n",
        "learning_rate = 0.001 #0.001\n",
        "epochs = 30        # 增加 epochs 讓模型有更多訓練機會\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 檢查是否有 GPU 可用，有的話使用 GPU 加速\n",
        "\n",
        "model = TransformerAutoencoder(feature_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to(device)\n",
        "model = model.float().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate) # 使用 Adam 優化器\n",
        "# criterion = nn.HuberLoss(reduction='mean') # 使用均方誤差 (MSE) 作為重建誤差損失函數\n",
        "criterion = nn.MSELoss(reduction='mean') # 使用均方誤差 (MSE) 作為重建誤差損失函數"
      ],
      "metadata": {
        "id": "FOsTdVhHgxK7"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 載入資料並建立 DataLoader\n",
        "# ------------------------------------\n",
        "train_data, test_data, test_labels = load_and_prepare_data(\n",
        "    csv_file='/content/kaggle_data/creditcard.csv', # 假設你的 CSV 檔案名為 transaction_data.csv\n",
        "    test_size=100                    # 使用最後 100 rows 作為測試集\n",
        ")\n",
        "\n",
        "# 建立訓練集 DataLoader\n",
        "train_dataset = torch.utils.data.TensorDataset(train_data, train_data) # Autoencoder 的輸入和輸出都是相同的資料 (重建)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 建立測試集 DataLoader (注意: 測試集不需要 shuffle)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_data, test_data)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "8EvQO0H-Flj6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(\n",
        "    # Set the wandb entity where your project will be logged (generally your team name).\n",
        "    entity=\"my-team-name\",\n",
        "    # Set the wandb project where this run will be logged.\n",
        "    project=\"my-project\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "    config={\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"architecture\": \"TransformerAutoencoder\",\n",
        "        \"dataset\": \"mlg-ulb/creditcardfraud\",\n",
        "        \"epochs\": epochs,\n",
        "    },\n",
        ")\n",
        "\n",
        "wandb.init()\n",
        "!wandb login #822c95d929b2b454ea30fa73027dda0d9eee6258"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ipa-wn3SkQ90",
        "outputId": "d2e0ce08-52e3-4c6f-d9aa-a4313aa49246"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mr06341047\u001b[0m (\u001b[33mr06341047-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 訓練模型 (Fine-tuning)\n",
        "# ------------------------------------\n",
        "history = {'train_loss': []} # 紀錄訓練過程中的 loss\n",
        "scaler = torch.cuda.amp.GradScaler() # 建立 GradScaler\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # 設定模型為訓練模式\n",
        "    train_loss = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "        inputs = inputs.unsqueeze(1).to(device).float() # 將輸入資料增加一個 sequence length 維度 (batch_size, seq_len=1, feature_size), 並移動到 GPU (如果有的話)\n",
        "        targets = targets.unsqueeze(1).to(device).float() # 同樣處理 targets\n",
        "\n",
        "        optimizer.zero_grad() # 清空梯度\n",
        "        with torch.cuda.amp.autocast(): # 在 autocast 範圍內執行 forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward() # 使用 scaler.scale(loss) 進行反向傳播\n",
        "        scaler.step(optimizer)        # 使用 scaler.step(optimizer) 更新參數\n",
        "        scaler.update()              # 更新 scaler\n",
        "        train_loss += loss.item() # 累加 batch loss\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader) # 計算平均 train loss\n",
        "    history['train_loss'].append(avg_train_loss)      # 紀錄平均 train loss\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "Rb0j4TqWuF9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 5. 訓練模型 (Fine-tuning)\n",
        "# # ------------------------------------\n",
        "# history = {'train_loss': []} # 紀錄訓練過程中的 loss\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     model.train() # 設定模型為訓練模式\n",
        "#     train_loss = 0.0\n",
        "#     for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "#         inputs = inputs.unsqueeze(1).to(device).float() # 將輸入資料增加一個 sequence length 維度 (batch_size, seq_len=1, feature_size), 並移動到 GPU (如果有的話)\n",
        "#         targets = targets.unsqueeze(1).to(device).float() # 同樣處理 targets\n",
        "\n",
        "#         optimizer.zero_grad() # 清空梯度\n",
        "#         outputs = model(inputs) # 前向傳播\n",
        "#         loss = criterion(outputs, targets) # 計算 loss (重建誤差)\n",
        "#         loss.backward()         # 反向傳播\n",
        "#         optimizer.step()        # 更新模型參數\n",
        "\n",
        "#         train_loss += loss.item() # 累加 batch loss\n",
        "\n",
        "#     avg_train_loss = train_loss / len(train_dataloader) # 計算平均 train loss\n",
        "#     history['train_loss'].append(avg_train_loss)      # 紀錄平均 train loss\n",
        "#     print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "l8-OeZkn-o0X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "c1616721-9aa3-4bf8-f336-53491be26456"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Train Loss: 106669444.1140\n",
            "Epoch [2/30], Train Loss: 76068941.0196\n",
            "Epoch [3/30], Train Loss: 75389443.8638\n",
            "Epoch [4/30], Train Loss: 75309199.8966\n",
            "Epoch [5/30], Train Loss: 75306596.4918\n",
            "Epoch [6/30], Train Loss: 75315582.3358\n",
            "Epoch [7/30], Train Loss: 75308588.4900\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-63c3da625d27>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 清空梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 前向傳播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 計算 loss (重建誤差)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# 反向傳播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-1b0f945de40c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Encoder 部分\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0menc_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 將輸入特徵轉換到 d_model 維度 [batch_size, seq_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_src\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# 通過 Encoder 層 [batch_size, seq_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 異常偵測和評估 (使用測試集)\n",
        "# ------------------------------------\n",
        "model.eval() # 設定模型為評估模式 (關閉 dropout 等)\n",
        "reconstruction_errors = [] # 儲存測試集樣本的重建誤差\n",
        "predicted_anomalies = [] # 儲存預測的異常標籤\n",
        "true_labels_list = []   # 儲存真實標籤 (如果有的話)\n",
        "\n",
        "with torch.no_grad(): # 在評估階段，不需要計算梯度\n",
        "    for inputs, targets in test_dataloader:\n",
        "        inputs = inputs.unsqueeze(1).to(device).float() # 處理輸入資料，增加 sequence length 維度並移動到 GPU\n",
        "        targets = targets.unsqueeze(1).to(device).float()\n",
        "        outputs = model(inputs) # 前向傳播，取得重建後的輸出\n",
        "        loss = criterion(outputs, targets) # 計算重建誤差 (MSE)\n",
        "        reconstruction_errors.extend(loss.cpu().numpy()) # 紀錄 batch 的平均重建誤差\n",
        "\n",
        "        # 簡單的異常判斷: 將重建誤差與閾值比較\n",
        "        threshold = np.percentile(reconstruction_errors, 85) # 使用重建誤差的 85 百分位數作為閾值 (可調整)\n",
        "        batch_predicted_anomalies = (loss.cpu().numpy() > threshold).astype(int) # 大於閾值判斷為異常 (1), 否則正常 (0)\n",
        "        predicted_anomalies.extend(batch_predicted_anomalies)\n",
        "\n",
        "        if test_labels is not None:\n",
        "            batch_true_labels = test_labels[len(true_labels_list): len(true_labels_list) + len(inputs)].numpy() # 取得當前 batch 的真實標籤\n",
        "            true_labels_list.extend(batch_true_labels) # 紀錄真實標籤\n",
        "\n",
        "\n",
        "# 將 reconstruction errors 轉換為每個樣本的誤差 (這裡簡化為 batch 平均誤差，實際應用中建議計算每個樣本的誤差)\n",
        "# 由於我們是計算 batch 的平均 MSE Loss, 這裡簡化處理，實際應用中建議調整 dataloader 每次只輸出 batch_size=1 來計算每個樣本的誤差\n",
        "reconstruction_errors_samples = np.repeat(np.array(reconstruction_errors), batch_size)[:len(test_data)] # 粗略將 batch error 擴展到樣本 (需調整)\n",
        "predicted_anomalies_samples = np.repeat(np.array(predicted_anomalies), batch_size)[:len(test_data)] # 粗略將 batch anomaly label 擴展到樣本\n",
        "\n",
        "\n",
        "# 評估模型 (如果測試集有真實標籤)\n",
        "if test_labels is not None:\n",
        "    print(\"\\n--- 評估結果 (基於測試集真實標籤) ---\")\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(true_labels_list[:len(predicted_anomalies_samples)], predicted_anomalies_samples)) # 注意 label 長度對齊\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(true_labels_list[:len(predicted_anomalies_samples)], predicted_anomalies_samples))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJ2WsXVOO_Aw",
        "outputId": "5e9f55e8-ff9d-4c66-ac1e-68f2da099438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name amu/tao-8k. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "), model_name='amu/tao-8k', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, multi_process=False, show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 繪製訓練 Loss 曲線 和 異常分數分佈 (可選)\n",
        "# ------------------------------------\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(reconstruction_errors_samples, bins=50)\n",
        "plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.2f})')\n",
        "plt.title('Test Reconstruction Error Distribution')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- 程式碼執行完畢 ---\")\n",
        "print(\"請查看訓練 Loss 曲線、測試集重建誤差分佈、以及評估結果 (如果測試集有真實標籤)。\")\n",
        "print(\"異常判斷閾值設定為重建誤差的 85 百分位數，您可以調整這個閾值來改變異常偵測的靈敏度。\")"
      ],
      "metadata": {
        "id": "KNNdYOIq_Nip",
        "outputId": "b6faa34f-eab9-4aec-97d0-65cbccfd5605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "共 476 頁\n",
            "最後一頁內容:  page_content='當之教育及培訓，使員工能適應 AI帶來之變革，並盡\\n可能維護其 工作權益。' metadata={'source': './ai.pdf', 'page': 25}\n",
            "全部內容:  現象，保護自然環境，從而促進包容性成長、永續發\n",
            "展及社會福祉。 (二)金融機構在 AI系統運用過程中， 宜對一般員工 提供適 當之教育及培訓，使員工能適應 AI帶來之變革，並盡\n",
            "可能維護其 工作權益。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yvp1br55VJnM",
        "outputId": "4c7ef7cb-260b-4387-92a8-a47b9f4a58e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " '。 (二)金融機構 使用 AI與消費者直接互動 時，應適當揭露 。 \\n六、促進永續發展 (一)金融機構在運用 AI系統時，應確保其發展策略及執行 與永續發展之原則相結合 ，包括減少經濟、社會等不平 等現象，保護自然環境，從而促進包容性成長、永續發\\n展及社會福祉。 (二)金融機構在 AI系統運用過程中， 宜對一般員工 提供適 當之教育及培訓，使員工能適應 AI帶來之變革，並盡\\n可能維護其 工作權益。')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.vectorstores import FAISS\n",
        "# results = db.similarity_search(\"\", k=3)\n",
        "# for i in range(len(results)):\n",
        "#   print(results[i])\n",
        "#   print('='*10)\n",
        "# # results[-1]\n",
        "\n",
        "# retriever = db.as_retriever(search_type=\"mmr\")\n",
        "# docs = retriever.get_relevant_documents(\"what about SupTech\")\n",
        "# docs"
      ],
      "metadata": {
        "id": "6bu2-_Uj-o5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "template = \"\"\"\n",
        "role: 你是台灣的金融監督管理委員會之「窗口」，並且說著正體中文\n",
        "question: {question}\n",
        "answer: 回覆之前請檢視自己的答案，不可以捏照回答。並且詳盡可能回答你的客戶\n",
        "context: {context}\n",
        "temperature=0.0\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,retriever=db.as_retriever(search_kwargs={'k':2}), chain_type_kwargs={\"prompt\": qa_prompt}, return_source_documents=True)\n",
        "result = qa_chain({\"question\": myquery, \"context\": myapp})\n",
        "\n",
        "\n",
        "print(qa_chain)\n",
        "\n",
        "\n",
        "res=llm.invoke(prompt)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "Uy911136d4SJ",
        "outputId": "fe52d982-a057-4382-caa6-06ebb0d10aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'myquery' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0882566bf15d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mqa_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqa_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_source_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmyquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmyapp\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_chain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'myquery' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E59Y6cWbd4VA",
        "outputId": "c169788d-c9bc-4940-86d8-ecf4246f7321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**您好！我是台灣金融監督管理委員會的「窗口」，我專門為您解答有關監理科技 (SupTech) 的相關問題。**\n",
            "\n",
            "**SuperTech 的定義是什麼？**\n",
            "\n",
            "SuperTech 是指利用數碼技術和網路技術來建立和營運金融產品和服務的領域。\n",
            "\n",
            "**SuperTech 的監理範圍是什麼？**\n",
            "\n",
            "SuperTech 的監理範圍涵蓋但不限於：\n",
            "\n",
            "* 產品設計與開發\n",
            "* 交易處理\n",
            "* 投資管理\n",
            "* 客戶服務\n",
            "* 安全與合規性\n",
            "\n",
            "**SuperTech 的監理義務是什麼？**\n",
            "\n",
            "SuperTech 的監理義務包括：\n",
            "\n",
            "* 定期監控產品和服務的設計與開發過程\n",
            "* 定期監控交易處理的過程\n",
            "* 定期監控投資管理的過程\n",
            "* 定期監控客戶服務的過程\n",
            "* 定期監控安全與合規性的狀況\n",
            "* 採取適當的措施來確保產品和服務安全與合規\n",
            "\n",
            "**如何遵守 SuperTech 的監理義務？**\n",
            "\n",
            "SuperTech 的監理義務可以透過以下方式實現：\n",
            "\n",
            "* 確保產品和服務的設計與開發過程符合安全與合規標準\n",
            "* 建立嚴格的交易處理流程\n",
            "* 建立完善的投資管理制度\n",
            "* 建立嚴格的客戶服務系統\n",
            "* 建立完善的安全與合規性管理系統\n",
            "\n",
            "**如何與 SuperTech 的監理相關聯繫？**\n",
            "\n",
            "您可以透過以下方式與 SuperTech 的監理相關聯繫：\n",
            "\n",
            "* 金管會網站上的資訊中心\n",
            "* 金管會辦理的監理報告\n",
            "* 金管會辦理的公開會議\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZ-0kH0fd4XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjy8zQacd4Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v82g7Zp8-I-m"
      },
      "source": [
        "## Test Library Setup\n",
        "\n",
        "Next, let's create our test \"library.\"\n",
        "\n",
        "For simplicity's sake, let's say that our \"library\" is simply a **nested directory of `.epub` files**. We can easily see this solution generalizing to, say, a Calibre library with a `metadata.db` database file. We'll leave that extension as an exercise for the reader. 😇\n",
        "\n",
        "Let's pull two `.epub` files from [Project Gutenberg](https://www.gutenberg.org/) for our library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie45lzO0-I-n"
      },
      "outputs": [],
      "source": [
        "!mkdir -p \".test/library/jane-austen\"\n",
        "!mkdir -p \".test/library/victor-hugo\"\n",
        "!wget https://www.gutenberg.org/ebooks/1342.epub.noimages -O \".test/library/jane-austen/pride-and-prejudice.epub\"\n",
        "!wget https://www.gutenberg.org/ebooks/135.epub.noimages -O \".test/library/victor-hugo/les-miserables.epub\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYp-HAmN-I-n"
      },
      "source": [
        "## RAG with LlamaIndex\n",
        "\n",
        "RAG with LlamaIndex, at its core, consists of the following broad phases:\n",
        "\n",
        "1. **Loading**, in which you tell LlamaIndex where your data lives and how to\n",
        "   load it;\n",
        "2. **Indexing**, in which you augment your loaded data to facilitate querying, e.g. with vector embeddings;\n",
        "3. **Querying**, in which you configure an LLM to act as the query interface for\n",
        "   your indexed data.\n",
        "\n",
        "This explanation only scratches at the surface of what's possible with\n",
        "LlamaIndex. For more in-depth details, I highly recommend reading the\n",
        "[\"High-Level Concepts\" page of the LlamaIndex\n",
        "documentation](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8QV16yJ-I-o"
      },
      "source": [
        "### Loading\n",
        "\n",
        "Naturally, let's start with the **loading** phase.\n",
        "\n",
        "I mentioned before that LlamaIndex is designed specifically for RAG. This\n",
        "immediately becomes obvious from its\n",
        "[`SimpleDirectoryReader`](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader.html)\n",
        "construct, which ✨ **magically** ✨ supports a whole host of multi-model file\n",
        "types for free. Conveniently for us, `.epub` is in the supported set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "url8lH9n-I-o"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "loader = SimpleDirectoryReader(\n",
        "    input_dir=\"./.test/\",\n",
        "    recursive=True,\n",
        "    required_exts=[\".epub\"],\n",
        ")\n",
        "\n",
        "documents = loader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWiY0D4-I-p"
      },
      "source": [
        "`SimpleDirectoryReader.load_data()` converts our ebooks into a set of [`Document`s](https://docs.llamaindex.ai/en/stable/api/llama_index.core.schema.Document.html) for LlamaIndex to work with.\n",
        "\n",
        "One important thing to note here is that the documents **have not been chunked at this stage** -- that will happen during indexing. Read on..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mDtpTdn-I-p"
      },
      "source": [
        "### Indexing\n",
        "\n",
        "Next up after **loading** the data is to **index** it. This will allow our RAG pipeline to look up the relevant context for our query to pass to our LLM to **augment** their generated response. This is also where document chunking will take place.\n",
        "\n",
        "[`VectorStoreIndex`](https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index.html)\n",
        "is a \"default\" entrypoint for indexing in LlamaIndex. By default,\n",
        "`VectorStoreIndex` uses a simple, in-memory dictionary to store the indices, but\n",
        "LlamaIndex also supports [a wide variety of vector storage\n",
        "solutions](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html)\n",
        "for you to graduate to as you scale.\n",
        "\n",
        "<Tip>\n",
        "By default, LlamaIndex uses a chunk size of 1024 and a chunk overlap of\n",
        "20. For more details, see the [LlamaIndex\n",
        "documentation](https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies.html#chunk-sizes).\n",
        "</Tip>\n",
        "\n",
        "\n",
        "Like mentioned before, we'll use the\n",
        "[`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) to\n",
        "generate our embeddings. By default, [LlamaIndex uses\n",
        "OpenAI](https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html)\n",
        "(specifically `gpt-3.5-turbo`), which we'd like to avoid given our desire for a lightweight, locally-runnable end-to-end solution.\n",
        "\n",
        "Thankfully, LlamaIndex supports retrieving embedding models from Hugging Face through the convenient `HuggingFaceEmbedding` class, so we'll use that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkJoR1xo-I-p"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mwPjFGz-I-p"
      },
      "source": [
        "We'll pass that in to `VectorStoreIndex` as our embedding model to circumvent the OpenAI default behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK5u69zP-I-p"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=embedding_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuEPvBTQ-I-p"
      },
      "source": [
        "### Querying\n",
        "\n",
        "Now for the final piece of the RAG puzzle -- wiring up the query layer.\n",
        "\n",
        "We'll use Llama 2 for the purposes of this recipe, but I encourage readers to play around with different models to see which produces the \"best\" responses here.\n",
        "\n",
        "First let's start up the Ollama server. Unfortunately, there is no support in the [Ollama Python client](https://github.com/ollama/ollama-python) for actually starting and stopping the server itself, so we'll have to pop out of Python land for this.\n",
        "\n",
        "In a separate terminal, run: `ollama serve`. Remember to terminate this after we're done here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sohcJ3cQ-I-q"
      },
      "source": [
        "Now let's hook Llama 2 up to LlamaIndex and use it as the basis of our query engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqlmG5mZ-I-q"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "llama = Ollama(\n",
        "    model=\"llama2\",\n",
        "    request_timeout=40.0,\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llama)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca8Gyur8-I-q"
      },
      "source": [
        "## Final Result\n",
        "\n",
        "With that, our basic RAG librarian is set up and we can start asking questions about our library. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvSrYLOq-I-q",
        "outputId": "415cfcdf-ddc3-4c68-f454-cde83b798a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the context provided, there are two books available:\n",
            "\n",
            "1. \"Pride and Prejudice\" by Jane Austen\n",
            "2. \"Les Misérables\" by Victor Hugo\n",
            "\n",
            "The context used to derive this answer includes:\n",
            "\n",
            "* The file path for each book, which provides information about the location of the book files on the computer.\n",
            "* The titles of the books, which are mentioned in the context as being available for reading.\n",
            "* A list of words associated with each book, such as \"epub\" and \"notebooks\", which provide additional information about the format and storage location of each book.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"What are the titles of all the books available? Show me the context used to derive your answer.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9gYtakU-I-q",
        "outputId": "3a90cb94-0de9-4af7-900a-18dcd9d3cf3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The main character of 'Pride and Prejudice' is Elizabeth Bennet.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"Who is the main character of 'Pride and Prejudice'?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYqlu4AX-I-r"
      },
      "source": [
        "## Conclusion and Future Improvements\n",
        "\n",
        "We've demonstrated how to build a basic RAG-based \"librarian\" that runs entirely locally, even on Apple silicon Macs. In doing so, we've also carried out a \"grand tour\" of LlamaIndex and how it streamlines the process of setting up RAG-based applications.\n",
        "\n",
        "That said, we've really only scratched the surface of what's possible here. Here are some ideas of how to refine and build upon this foundation.\n",
        "\n",
        "### Forcing Citations\n",
        "\n",
        "To guard against the risk of our librarian hallucinating, how might we require that it provide citations for everything that it says?\n",
        "\n",
        "### Using Extended Metadata\n",
        "\n",
        "Ebook library management solutions like [Calibre](https://calibre-ebook.com/) create additional metadata for ebooks in a library. This can provide information such as publisher or edition that might not be readily available in the text of the book itself. How could we extend our RAG pipeline to account for additional sources of information that aren't `.epub` files?\n",
        "\n",
        "### Efficient Indexing\n",
        "\n",
        "If we were to collect everything we built here into a script/executable, the resulting script would re-index our library on each invocation. For our tiny test library of two files, this is \"fine,\" but for any library of non-trivial size this will very quickly become annoying for users. How could we persist the embedding indices and only update them when the contents of the library have meaningfully changed, e.g. new books have been added?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}