{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhe0/prac/blob/main/simple_rag_by_gguf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rag chatbot example\n",
        "* åå°çš„èªè¨€æ¨¡çµ„\n",
        "* æ¸›å°‘å¹»è¦ºç¨‹åº¦"
      ],
      "metadata": {
        "id": "ZJbsrUlcYUAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "!pip install -q kaggle\n",
        "!chmod 600 ~/kaggle.json # è¨­å®šæ¬Šé™ï¼Œç¢ºä¿åªæœ‰ä½¿ç”¨è€…å¯ä»¥è®€å–\n",
        "!cp kaggle.json /root/.config/kaggle/ # move the key to the folder\n",
        "import kaggle\n",
        "print(\"Kaggle API æ†‘è­‰è¨­å®šå®Œæˆï¼\")\n",
        "\n",
        "\n",
        "# 3. ä¸‹è¼‰ Kaggle Dataset\n",
        "# ------------------------------------\n",
        "# **é‡è¦:** ä½ éœ€è¦çŸ¥é“ä½ è¦ä¸‹è¼‰çš„ Kaggle Dataset çš„åç¨± (é€šå¸¸æ˜¯ 'owner/dataset-name')\n",
        "# ä¾‹å¦‚ï¼Œå¦‚æœè¦ä¸‹è¼‰ 'creditcardfraud' é€™å€‹è³‡æ–™é›† (ç¤ºç¯„ç”¨ï¼Œä½ å¯ä»¥æ›¿æ›æˆä½ çš„è³‡æ–™é›†)\n",
        "dataset_name = 'mlg-ulb/creditcardfraud' #  <--  è«‹åœ¨é€™è£¡æ›¿æ›æˆä½ è¦ä¸‹è¼‰çš„ Kaggle Dataset åç¨±\n",
        "\n",
        "output_path = './kaggle_data' # è¨­å®šè³‡æ–™é›†ä¸‹è¼‰çš„è·¯å¾‘ (Colab notebook çš„ç•¶å‰ç›®éŒ„ä¸‹çš„ kaggle_data è³‡æ–™å¤¾)\n",
        "!mkdir -p {output_path} # å»ºç«‹è³‡æ–™å¤¾ (å¦‚æœä¸å­˜åœ¨)\n",
        "\n",
        "print(f\"æ­£åœ¨ä¸‹è¼‰ Kaggle Dataset: {dataset_name} åˆ° {output_path}...\")\n",
        "!kaggle datasets download -d {dataset_name} -p {output_path}\n",
        "print(\"Kaggle Dataset ä¸‹è¼‰å®Œæˆï¼\")\n",
        "\n",
        "\n",
        "# 4. è§£å£“ç¸®ä¸‹è¼‰çš„è³‡æ–™é›† (å¦‚æœéœ€è¦)\n",
        "# ------------------------------------\n",
        "# Kaggle Dataset é€šå¸¸æœƒä¸‹è¼‰æˆ zip æª”æ¡ˆï¼Œéœ€è¦è§£å£“ç¸®æ‰èƒ½è®€å–è£¡é¢çš„è³‡æ–™æª”æ¡ˆ\n",
        "import zipfile\n",
        "\n",
        "zip_file_path = os.path.join(output_path, f'{dataset_name.split(\"/\")[1]}.zip') # å‡è¨­ä¸‹è¼‰çš„ zip æª”åèˆ‡ dataset name ç›¸é—œ\n",
        "extract_path = output_path # è§£å£“ç¸®åˆ°åŒä¸€å€‹ output_path è³‡æ–™å¤¾\n",
        "\n",
        "print(f\"æ­£åœ¨è§£å£“ç¸®æª”æ¡ˆ: {zip_file_path} åˆ° {extract_path}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"è³‡æ–™é›†è§£å£“ç¸®å®Œæˆã€‚\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"æ‰¾ä¸åˆ° zip æª”æ¡ˆ: {zip_file_path}ã€‚ å¦‚æœä¸‹è¼‰çš„ä¸æ˜¯ zip æª”æ¡ˆï¼Œè«‹è·³éæ­¤æ­¥é©Ÿã€‚\")\n",
        "\n",
        "\n",
        "# 5. è®€å–è³‡æ–™åˆ° Pandas DataFrame (å‡è¨­è³‡æ–™æ˜¯ CSV æ ¼å¼)\n",
        "# ------------------------------------\n",
        "csv_file_name = 'creditcard.csv' # <-- è«‹æ ¹æ“šä½ çš„è³‡æ–™é›†ä¿®æ”¹ CSV æª”æ¡ˆåç¨± (å¦‚æœé©ç”¨)\n",
        "csv_file_path = os.path.join(extract_path, csv_file_name) # å‡è¨­ CSV æª”æ¡ˆåœ¨è§£å£“ç¸®å¾Œçš„è³‡æ–™å¤¾ä¸­\n",
        "\n",
        "print(f\"æ­£åœ¨è®€å– CSV æª”æ¡ˆ: {csv_file_path}...\")\n",
        "try:\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "    print(\"CSV æª”æ¡ˆè®€å–æˆåŠŸï¼ŒPandas DataFrame è³‡è¨Š:\")\n",
        "    print(df.head()) # å°å‡º DataFrame çš„å‰å¹¾è¡Œ\n",
        "    print(df.info()) # å°å‡º DataFrame çš„è³‡è¨Š (æ¬„ä½åç¨±, è³‡æ–™å‹åˆ¥, éç©ºå€¼æ•¸é‡ç­‰)\n",
        "\n",
        "    # 6. å°‡ Pandas DataFrame è½‰æ›æˆ PyTorch Tensor (ç¯„ä¾‹)\n",
        "    # ------------------------------------\n",
        "    # # å‡è¨­ä½ è¦ä½¿ç”¨ 'V1', 'V2', 'Amount' é€™äº›æ¬„ä½ä½œç‚ºç‰¹å¾µ\n",
        "    # feature_cols = ['V1', 'V2', 'Amount'] # <-- è«‹æ ¹æ“šä½ çš„éœ€æ±‚ä¿®æ”¹ç‰¹å¾µæ¬„ä½\n",
        "    features = df[[i for i in df.columns if not i == 'Class']].values # å–å¾—ç‰¹å¾µå€¼ (NumPy array)\n",
        "    features_tensor = torch.tensor(features, dtype=torch.float32) # è½‰æ›æˆ PyTorch FloatTensor\n",
        "\n",
        "    # å¦‚æœæœ‰ 'Class' æ¬„ä½ä½œç‚ºæ¨™ç±¤\n",
        "    if 'Class' in df.columns:\n",
        "        labels = df['Class'].values # å–å¾—æ¨™ç±¤å€¼ (NumPy array)\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.long) # è½‰æ›æˆ PyTorch LongTensor (æ•´æ•¸æ¨™ç±¤)\n",
        "        print(\"\\nç‰¹å¾µ Tensor (features_tensor) shape:\", features_tensor.shape)\n",
        "        print(\"æ¨™ç±¤ Tensor (labels_tensor) shape:\", labels_tensor.shape)\n",
        "    else:\n",
        "        print(\"\\nç‰¹å¾µ Tensor (features_tensor) shape:\", features_tensor.shape)\n",
        "        print(\"æ²’æœ‰ 'Class' æ¬„ä½ï¼Œåƒ…è½‰æ›ç‰¹å¾µ Tensorã€‚\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- å¾ Kaggle Dataset è®€å–è³‡æ–™æµç¨‹å®Œæˆ ---\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"æ‰¾ä¸åˆ° CSV æª”æ¡ˆ: {csv_file_path}ã€‚ è«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘å’Œæª”æ¡ˆåç¨±æ˜¯å¦æ­£ç¢ºã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"è®€å– CSV æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n"
      ],
      "metadata": {
        "id": "QGye1ERE-e5m",
        "outputId": "4698e812-40bd-4239-d663-68f24da341c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/kaggle.json': No such file or directory\n",
            "Kaggle API æ†‘è­‰è¨­å®šå®Œæˆï¼\n",
            "æ­£åœ¨ä¸‹è¼‰ Kaggle Dataset: mlg-ulb/creditcardfraud åˆ° ./kaggle_data...\n",
            "Dataset URL: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
            "License(s): DbCL-1.0\n",
            "creditcardfraud.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Kaggle Dataset ä¸‹è¼‰å®Œæˆï¼\n",
            "æ­£åœ¨è§£å£“ç¸®æª”æ¡ˆ: ./kaggle_data/creditcardfraud.zip åˆ° ./kaggle_data...\n",
            "è³‡æ–™é›†è§£å£“ç¸®å®Œæˆã€‚\n",
            "æ­£åœ¨è®€å– CSV æª”æ¡ˆ: ./kaggle_data/creditcard.csv...\n",
            "CSV æª”æ¡ˆè®€å–æˆåŠŸï¼ŒPandas DataFrame è³‡è¨Š:\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n",
            "\n",
            "ç‰¹å¾µ Tensor (features_tensor) shape: torch.Size([284807, 30])\n",
            "æ¨™ç±¤ Tensor (labels_tensor) shape: torch.Size([284807])\n",
            "\n",
            "--- å¾ Kaggle Dataset è®€å–è³‡æ–™æµç¨‹å®Œæˆ ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self, feature_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding_encoder = nn.Linear(feature_size, d_model) # è¼¸å…¥ç‰¹å¾µ Embedding å±¤\n",
        "        self.embedding_decoder = nn.Linear(d_model, feature_size) # è¼¸å‡ºç‰¹å¾µ De-embedding å±¤ (ç”¨æ–¼é‡å»º)\n",
        "\n",
        "        # Encoder å±¤å †ç–Š\n",
        "        encoder_layers = [TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_encoder_layers)]\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Decoder å±¤å †ç–Š\n",
        "        decoder_layers = [TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_decoder_layers)]\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Encoder éƒ¨åˆ†\n",
        "        enc_src = self.embedding_encoder(src) # å°‡è¼¸å…¥ç‰¹å¾µè½‰æ›åˆ° d_model ç¶­åº¦ [batch_size, seq_len, d_model]\n",
        "        memory = self.encoder(enc_src)       # é€šé Encoder å±¤ [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Decoder éƒ¨åˆ† - æ­£ç¢ºçš„ Sequential è¿­ä»£æ–¹å¼\n",
        "        tgt = memory # åˆå§‹åŒ– decoder çš„ input ç‚º encoder çš„ memory (å°æ–¼ Autoencoder ä¾†èªªï¼Œé€šå¸¸å°‡ encoder çš„è¼¸å‡ºä½œç‚º decoder çš„åˆå§‹è¼¸å…¥)\n",
        "        for decoder_layer in self.decoder: # è¿­ä»£ self.decoder (nn.Sequential å®¹å™¨) ä¸­çš„æ¯ä¸€å±¤\n",
        "            tgt = decoder_layer(tgt, memory) # é¡¯å¼èª¿ç”¨æ¯å€‹ decoder_layer çš„ forward æ–¹æ³•ï¼Œä¸¦å‚³é tgt å’Œ memory\n",
        "\n",
        "        output = self.embedding_decoder(tgt) # [batch_size, seq_len, feature_size]\n",
        "        return output"
      ],
      "metadata": {
        "id": "zkh34SESSg2y"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "F53QV6f6-I-l"
      },
      "outputs": [],
      "source": [
        "# 2. å¾ CSV æª”æ¡ˆè¼‰å…¥è³‡æ–™ä¸¦æº–å‚™è¨“ç·´/æ¸¬è©¦é›†\n",
        "# ------------------------------------\n",
        "def load_and_prepare_data(csv_file='transaction_data.csv', feature_cols=None, test_size=100):\n",
        "    \"\"\"\n",
        "    å¾ CSV æª”æ¡ˆè¼‰å…¥è³‡æ–™ï¼Œé¸æ“‡ç‰¹å¾µæ¬„ä½ï¼Œä¸¦åˆ†å‰²æˆè¨“ç·´é›†å’Œæ¸¬è©¦é›†ã€‚\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): CSV æª”æ¡ˆè·¯å¾‘ (å‡è¨­å·²ä¸Šå‚³åˆ° Colab).\n",
        "        feature_cols (list): è¦ä½¿ç”¨çš„ç‰¹å¾µæ¬„ä½åç¨±åˆ—è¡¨.\n",
        "        test_size (int): æ¸¬è©¦é›†å¤§å° (å–æœ€å¾Œå¹¾ rows ä½œç‚ºæ¸¬è©¦é›†).\n",
        "\n",
        "    Returns:\n",
        "        tuple: è¨“ç·´é›† (torch.Tensor), æ¸¬è©¦é›† (torch.Tensor), çœŸå¯¦æ¨™ç±¤ (æ¸¬è©¦é›†, torch.Tensor, å¦‚æœ CSV åŒ…å« 'Class' æ¬„ä½).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # é¸æ“‡ç‰¹å¾µæ¬„ä½\n",
        "    if feature_cols is None:\n",
        "        feature_cols = [i for i in df.columns if not i == 'Class'] # å‡è¨­ V1, V2, Amount éƒ½æ˜¯ç‰¹å¾µæ¬„ä½ï¼Œæˆ–è€…ä½¿ç”¨å…¨éƒ¨\n",
        "    X = df[feature_cols].values\n",
        "    y = None # é è¨­æ²’æœ‰æ¨™ç±¤\n",
        "    if 'Class' in df.columns:\n",
        "        y = df['Class'].values # å¦‚æœ CSV åŒ…å« 'Class' æ¬„ä½ï¼Œå‰‡è®€å–æ¨™ç±¤\n",
        "\n",
        "    # å°‡è³‡æ–™è½‰æ›æˆ PyTorch tensors\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    if y is not None:\n",
        "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "    else:\n",
        "        y_tensor = None\n",
        "\n",
        "    # åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›† (å–æœ€å¾Œ test_size rows ä½œç‚ºæ¸¬è©¦é›†)\n",
        "    train_data = X_tensor[:-test_size]\n",
        "    test_data = X_tensor[-test_size:]\n",
        "    test_labels = None\n",
        "    if y_tensor is not None:\n",
        "        test_labels = y_tensor[-test_size:]\n",
        "\n",
        "    return train_data, test_data, test_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. è¨­å®šè¶…åƒæ•¸å’Œæ¨¡å‹ã€å„ªåŒ–å™¨ã€æå¤±å‡½æ•¸\n",
        "# ------------------------------------\n",
        "feature_size = len(df.columns)-1 # è¼¸å…¥ç‰¹å¾µç¶­åº¦2 (V1, V2)\n",
        "sequence_length = 1 # æ¯å€‹æ¨£æœ¬è¦–ç‚ºé•·åº¦ç‚º 1 çš„åºåˆ— (å› ç‚ºæˆ‘å€‘ç›®å‰æ˜¯ç¨ç«‹è™•ç†æ¯å€‹ row)\n",
        "d_model = 64       # Transformer æ¨¡å‹ä¸­çš„ embedding dimension\n",
        "nhead = 2          # Multi-head attention head æ•¸é‡\n",
        "num_encoder_layers = 2 # Encoder å±¤æ•¸\n",
        "num_decoder_layers = 2 # Decoder å±¤æ•¸\n",
        "dim_feedforward = 128 # Feedforward network hidden layer dimension\n",
        "dropout = 0.1\n",
        "learning_rate = 0.001 #0.001\n",
        "epochs = 30        # å¢åŠ  epochs è®“æ¨¡å‹æœ‰æ›´å¤šè¨“ç·´æ©Ÿæœƒ\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # æª¢æŸ¥æ˜¯å¦æœ‰ GPU å¯ç”¨ï¼Œæœ‰çš„è©±ä½¿ç”¨ GPU åŠ é€Ÿ\n",
        "\n",
        "model = TransformerAutoencoder(feature_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to(device)\n",
        "model = model.float().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate) # ä½¿ç”¨ Adam å„ªåŒ–å™¨\n",
        "# criterion = nn.HuberLoss(reduction='mean') # ä½¿ç”¨å‡æ–¹èª¤å·® (MSE) ä½œç‚ºé‡å»ºèª¤å·®æå¤±å‡½æ•¸\n",
        "criterion = nn.MSELoss(reduction='mean') # ä½¿ç”¨å‡æ–¹èª¤å·® (MSE) ä½œç‚ºé‡å»ºèª¤å·®æå¤±å‡½æ•¸"
      ],
      "metadata": {
        "id": "FOsTdVhHgxK7"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. è¼‰å…¥è³‡æ–™ä¸¦å»ºç«‹ DataLoader\n",
        "# ------------------------------------\n",
        "train_data, test_data, test_labels = load_and_prepare_data(\n",
        "    csv_file='/content/kaggle_data/creditcard.csv', # å‡è¨­ä½ çš„ CSV æª”æ¡ˆåç‚º transaction_data.csv\n",
        "    test_size=100                    # ä½¿ç”¨æœ€å¾Œ 100 rows ä½œç‚ºæ¸¬è©¦é›†\n",
        ")\n",
        "\n",
        "# å»ºç«‹è¨“ç·´é›† DataLoader\n",
        "train_dataset = torch.utils.data.TensorDataset(train_data, train_data) # Autoencoder çš„è¼¸å…¥å’Œè¼¸å‡ºéƒ½æ˜¯ç›¸åŒçš„è³‡æ–™ (é‡å»º)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# å»ºç«‹æ¸¬è©¦é›† DataLoader (æ³¨æ„: æ¸¬è©¦é›†ä¸éœ€è¦ shuffle)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_data, test_data)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "8EvQO0H-Flj6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(\n",
        "    # Set the wandb entity where your project will be logged (generally your team name).\n",
        "    entity=\"my-team-name\",\n",
        "    # Set the wandb project where this run will be logged.\n",
        "    project=\"my-project\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "    config={\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"architecture\": \"TransformerAutoencoder\",\n",
        "        \"dataset\": \"mlg-ulb/creditcardfraud\",\n",
        "        \"epochs\": epochs,\n",
        "    },\n",
        ")\n",
        "\n",
        "wandb.init()\n",
        "!wandb login #822c95d929b2b454ea30fa73027dda0d9eee6258"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ipa-wn3SkQ90",
        "outputId": "d2e0ce08-52e3-4c6f-d9aa-a4313aa49246"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mr06341047\u001b[0m (\u001b[33mr06341047-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. è¨“ç·´æ¨¡å‹ (Fine-tuning)\n",
        "# ------------------------------------\n",
        "history = {'train_loss': []} # ç´€éŒ„è¨“ç·´éç¨‹ä¸­çš„ loss\n",
        "scaler = torch.cuda.amp.GradScaler() # å»ºç«‹ GradScaler\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # è¨­å®šæ¨¡å‹ç‚ºè¨“ç·´æ¨¡å¼\n",
        "    train_loss = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "        inputs = inputs.unsqueeze(1).to(device).float() # å°‡è¼¸å…¥è³‡æ–™å¢åŠ ä¸€å€‹ sequence length ç¶­åº¦ (batch_size, seq_len=1, feature_size), ä¸¦ç§»å‹•åˆ° GPU (å¦‚æœæœ‰çš„è©±)\n",
        "        targets = targets.unsqueeze(1).to(device).float() # åŒæ¨£è™•ç† targets\n",
        "\n",
        "        optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦\n",
        "        with torch.cuda.amp.autocast(): # åœ¨ autocast ç¯„åœå…§åŸ·è¡Œ forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward() # ä½¿ç”¨ scaler.scale(loss) é€²è¡Œåå‘å‚³æ’­\n",
        "        scaler.step(optimizer)        # ä½¿ç”¨ scaler.step(optimizer) æ›´æ–°åƒæ•¸\n",
        "        scaler.update()              # æ›´æ–° scaler\n",
        "        train_loss += loss.item() # ç´¯åŠ  batch loss\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_dataloader) # è¨ˆç®—å¹³å‡ train loss\n",
        "    history['train_loss'].append(avg_train_loss)      # ç´€éŒ„å¹³å‡ train loss\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "Rb0j4TqWuF9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 5. è¨“ç·´æ¨¡å‹ (Fine-tuning)\n",
        "# # ------------------------------------\n",
        "# history = {'train_loss': []} # ç´€éŒ„è¨“ç·´éç¨‹ä¸­çš„ loss\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     model.train() # è¨­å®šæ¨¡å‹ç‚ºè¨“ç·´æ¨¡å¼\n",
        "#     train_loss = 0.0\n",
        "#     for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "#         inputs = inputs.unsqueeze(1).to(device).float() # å°‡è¼¸å…¥è³‡æ–™å¢åŠ ä¸€å€‹ sequence length ç¶­åº¦ (batch_size, seq_len=1, feature_size), ä¸¦ç§»å‹•åˆ° GPU (å¦‚æœæœ‰çš„è©±)\n",
        "#         targets = targets.unsqueeze(1).to(device).float() # åŒæ¨£è™•ç† targets\n",
        "\n",
        "#         optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦\n",
        "#         outputs = model(inputs) # å‰å‘å‚³æ’­\n",
        "#         loss = criterion(outputs, targets) # è¨ˆç®— loss (é‡å»ºèª¤å·®)\n",
        "#         loss.backward()         # åå‘å‚³æ’­\n",
        "#         optimizer.step()        # æ›´æ–°æ¨¡å‹åƒæ•¸\n",
        "\n",
        "#         train_loss += loss.item() # ç´¯åŠ  batch loss\n",
        "\n",
        "#     avg_train_loss = train_loss / len(train_dataloader) # è¨ˆç®—å¹³å‡ train loss\n",
        "#     history['train_loss'].append(avg_train_loss)      # ç´€éŒ„å¹³å‡ train loss\n",
        "#     print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "l8-OeZkn-o0X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "c1616721-9aa3-4bf8-f336-53491be26456"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Train Loss: 106669444.1140\n",
            "Epoch [2/30], Train Loss: 76068941.0196\n",
            "Epoch [3/30], Train Loss: 75389443.8638\n",
            "Epoch [4/30], Train Loss: 75309199.8966\n",
            "Epoch [5/30], Train Loss: 75306596.4918\n",
            "Epoch [6/30], Train Loss: 75315582.3358\n",
            "Epoch [7/30], Train Loss: 75308588.4900\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-63c3da625d27>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# æ¸…ç©ºæ¢¯åº¦\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# å‰å‘å‚³æ’­\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# è¨ˆç®— loss (é‡å»ºèª¤å·®)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# åå‘å‚³æ’­\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-1b0f945de40c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Encoder éƒ¨åˆ†\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0menc_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# å°‡è¼¸å…¥ç‰¹å¾µè½‰æ›åˆ° d_model ç¶­åº¦ [batch_size, seq_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_src\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# é€šé Encoder å±¤ [batch_size, seq_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. ç•°å¸¸åµæ¸¬å’Œè©•ä¼° (ä½¿ç”¨æ¸¬è©¦é›†)\n",
        "# ------------------------------------\n",
        "model.eval() # è¨­å®šæ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼ (é—œé–‰ dropout ç­‰)\n",
        "reconstruction_errors = [] # å„²å­˜æ¸¬è©¦é›†æ¨£æœ¬çš„é‡å»ºèª¤å·®\n",
        "predicted_anomalies = [] # å„²å­˜é æ¸¬çš„ç•°å¸¸æ¨™ç±¤\n",
        "true_labels_list = []   # å„²å­˜çœŸå¯¦æ¨™ç±¤ (å¦‚æœæœ‰çš„è©±)\n",
        "\n",
        "with torch.no_grad(): # åœ¨è©•ä¼°éšæ®µï¼Œä¸éœ€è¦è¨ˆç®—æ¢¯åº¦\n",
        "    for inputs, targets in test_dataloader:\n",
        "        inputs = inputs.unsqueeze(1).to(device).float() # è™•ç†è¼¸å…¥è³‡æ–™ï¼Œå¢åŠ  sequence length ç¶­åº¦ä¸¦ç§»å‹•åˆ° GPU\n",
        "        targets = targets.unsqueeze(1).to(device).float()\n",
        "        outputs = model(inputs) # å‰å‘å‚³æ’­ï¼Œå–å¾—é‡å»ºå¾Œçš„è¼¸å‡º\n",
        "        loss = criterion(outputs, targets) # è¨ˆç®—é‡å»ºèª¤å·® (MSE)\n",
        "        reconstruction_errors.extend(loss.cpu().numpy()) # ç´€éŒ„ batch çš„å¹³å‡é‡å»ºèª¤å·®\n",
        "\n",
        "        # ç°¡å–®çš„ç•°å¸¸åˆ¤æ–·: å°‡é‡å»ºèª¤å·®èˆ‡é–¾å€¼æ¯”è¼ƒ\n",
        "        threshold = np.percentile(reconstruction_errors, 85) # ä½¿ç”¨é‡å»ºèª¤å·®çš„ 85 ç™¾åˆ†ä½æ•¸ä½œç‚ºé–¾å€¼ (å¯èª¿æ•´)\n",
        "        batch_predicted_anomalies = (loss.cpu().numpy() > threshold).astype(int) # å¤§æ–¼é–¾å€¼åˆ¤æ–·ç‚ºç•°å¸¸ (1), å¦å‰‡æ­£å¸¸ (0)\n",
        "        predicted_anomalies.extend(batch_predicted_anomalies)\n",
        "\n",
        "        if test_labels is not None:\n",
        "            batch_true_labels = test_labels[len(true_labels_list): len(true_labels_list) + len(inputs)].numpy() # å–å¾—ç•¶å‰ batch çš„çœŸå¯¦æ¨™ç±¤\n",
        "            true_labels_list.extend(batch_true_labels) # ç´€éŒ„çœŸå¯¦æ¨™ç±¤\n",
        "\n",
        "\n",
        "# å°‡ reconstruction errors è½‰æ›ç‚ºæ¯å€‹æ¨£æœ¬çš„èª¤å·® (é€™è£¡ç°¡åŒ–ç‚º batch å¹³å‡èª¤å·®ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­å»ºè­°è¨ˆç®—æ¯å€‹æ¨£æœ¬çš„èª¤å·®)\n",
        "# ç”±æ–¼æˆ‘å€‘æ˜¯è¨ˆç®— batch çš„å¹³å‡ MSE Loss, é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­å»ºè­°èª¿æ•´ dataloader æ¯æ¬¡åªè¼¸å‡º batch_size=1 ä¾†è¨ˆç®—æ¯å€‹æ¨£æœ¬çš„èª¤å·®\n",
        "reconstruction_errors_samples = np.repeat(np.array(reconstruction_errors), batch_size)[:len(test_data)] # ç²—ç•¥å°‡ batch error æ“´å±•åˆ°æ¨£æœ¬ (éœ€èª¿æ•´)\n",
        "predicted_anomalies_samples = np.repeat(np.array(predicted_anomalies), batch_size)[:len(test_data)] # ç²—ç•¥å°‡ batch anomaly label æ“´å±•åˆ°æ¨£æœ¬\n",
        "\n",
        "\n",
        "# è©•ä¼°æ¨¡å‹ (å¦‚æœæ¸¬è©¦é›†æœ‰çœŸå¯¦æ¨™ç±¤)\n",
        "if test_labels is not None:\n",
        "    print(\"\\n--- è©•ä¼°çµæœ (åŸºæ–¼æ¸¬è©¦é›†çœŸå¯¦æ¨™ç±¤) ---\")\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(true_labels_list[:len(predicted_anomalies_samples)], predicted_anomalies_samples)) # æ³¨æ„ label é•·åº¦å°é½Š\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(true_labels_list[:len(predicted_anomalies_samples)], predicted_anomalies_samples))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJ2WsXVOO_Aw",
        "outputId": "5e9f55e8-ff9d-4c66-ac1e-68f2da099438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name amu/tao-8k. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "), model_name='amu/tao-8k', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, multi_process=False, show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. ç¹ªè£½è¨“ç·´ Loss æ›²ç·š å’Œ ç•°å¸¸åˆ†æ•¸åˆ†ä½ˆ (å¯é¸)\n",
        "# ------------------------------------\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(reconstruction_errors_samples, bins=50)\n",
        "plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.2f})')\n",
        "plt.title('Test Reconstruction Error Distribution')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- ç¨‹å¼ç¢¼åŸ·è¡Œå®Œç•¢ ---\")\n",
        "print(\"è«‹æŸ¥çœ‹è¨“ç·´ Loss æ›²ç·šã€æ¸¬è©¦é›†é‡å»ºèª¤å·®åˆ†ä½ˆã€ä»¥åŠè©•ä¼°çµæœ (å¦‚æœæ¸¬è©¦é›†æœ‰çœŸå¯¦æ¨™ç±¤)ã€‚\")\n",
        "print(\"ç•°å¸¸åˆ¤æ–·é–¾å€¼è¨­å®šç‚ºé‡å»ºèª¤å·®çš„ 85 ç™¾åˆ†ä½æ•¸ï¼Œæ‚¨å¯ä»¥èª¿æ•´é€™å€‹é–¾å€¼ä¾†æ”¹è®Šç•°å¸¸åµæ¸¬çš„éˆæ•åº¦ã€‚\")"
      ],
      "metadata": {
        "id": "KNNdYOIq_Nip",
        "outputId": "b6faa34f-eab9-4aec-97d0-65cbccfd5605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å…± 476 é \n",
            "æœ€å¾Œä¸€é å…§å®¹:  page_content='ç•¶ä¹‹æ•™è‚²åŠåŸ¹è¨“ï¼Œä½¿å“¡å·¥èƒ½é©æ‡‰ AIå¸¶ä¾†ä¹‹è®Šé©ï¼Œä¸¦ç›¡\\nå¯èƒ½ç¶­è­·å…¶ å·¥ä½œæ¬Šç›Šã€‚' metadata={'source': './ai.pdf', 'page': 25}\n",
            "å…¨éƒ¨å…§å®¹:  ç¾è±¡ï¼Œä¿è­·è‡ªç„¶ç’°å¢ƒï¼Œå¾è€Œä¿ƒé€²åŒ…å®¹æ€§æˆé•·ã€æ°¸çºŒç™¼\n",
            "å±•åŠç¤¾æœƒç¦ç¥‰ã€‚ (äºŒ)é‡‘èæ©Ÿæ§‹åœ¨ AIç³»çµ±é‹ç”¨éç¨‹ä¸­ï¼Œ å®œå°ä¸€èˆ¬å“¡å·¥ æä¾›é© ç•¶ä¹‹æ•™è‚²åŠåŸ¹è¨“ï¼Œä½¿å“¡å·¥èƒ½é©æ‡‰ AIå¸¶ä¾†ä¹‹è®Šé©ï¼Œä¸¦ç›¡\n",
            "å¯èƒ½ç¶­è­·å…¶ å·¥ä½œæ¬Šç›Šã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yvp1br55VJnM",
        "outputId": "4c7ef7cb-260b-4387-92a8-a47b9f4a58e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'ã€‚ (äºŒ)é‡‘èæ©Ÿæ§‹ ä½¿ç”¨ AIèˆ‡æ¶ˆè²»è€…ç›´æ¥äº’å‹• æ™‚ï¼Œæ‡‰é©ç•¶æ­éœ² ã€‚ \\nå…­ã€ä¿ƒé€²æ°¸çºŒç™¼å±• (ä¸€)é‡‘èæ©Ÿæ§‹åœ¨é‹ç”¨ AIç³»çµ±æ™‚ï¼Œæ‡‰ç¢ºä¿å…¶ç™¼å±•ç­–ç•¥åŠåŸ·è¡Œ èˆ‡æ°¸çºŒç™¼å±•ä¹‹åŸå‰‡ç›¸çµåˆ ï¼ŒåŒ…æ‹¬æ¸›å°‘ç¶“æ¿Ÿã€ç¤¾æœƒç­‰ä¸å¹³ ç­‰ç¾è±¡ï¼Œä¿è­·è‡ªç„¶ç’°å¢ƒï¼Œå¾è€Œä¿ƒé€²åŒ…å®¹æ€§æˆé•·ã€æ°¸çºŒç™¼\\nå±•åŠç¤¾æœƒç¦ç¥‰ã€‚ (äºŒ)é‡‘èæ©Ÿæ§‹åœ¨ AIç³»çµ±é‹ç”¨éç¨‹ä¸­ï¼Œ å®œå°ä¸€èˆ¬å“¡å·¥ æä¾›é© ç•¶ä¹‹æ•™è‚²åŠåŸ¹è¨“ï¼Œä½¿å“¡å·¥èƒ½é©æ‡‰ AIå¸¶ä¾†ä¹‹è®Šé©ï¼Œä¸¦ç›¡\\nå¯èƒ½ç¶­è­·å…¶ å·¥ä½œæ¬Šç›Šã€‚')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.vectorstores import FAISS\n",
        "# results = db.similarity_search(\"\", k=3)\n",
        "# for i in range(len(results)):\n",
        "#   print(results[i])\n",
        "#   print('='*10)\n",
        "# # results[-1]\n",
        "\n",
        "# retriever = db.as_retriever(search_type=\"mmr\")\n",
        "# docs = retriever.get_relevant_documents(\"what about SupTech\")\n",
        "# docs"
      ],
      "metadata": {
        "id": "6bu2-_Uj-o5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "template = \"\"\"\n",
        "role: ä½ æ˜¯å°ç£çš„é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒä¹‹ã€Œçª—å£ã€ï¼Œä¸¦ä¸”èªªè‘—æ­£é«”ä¸­æ–‡\n",
        "question: {question}\n",
        "answer: å›è¦†ä¹‹å‰è«‹æª¢è¦–è‡ªå·±çš„ç­”æ¡ˆï¼Œä¸å¯ä»¥æç…§å›ç­”ã€‚ä¸¦ä¸”è©³ç›¡å¯èƒ½å›ç­”ä½ çš„å®¢æˆ¶\n",
        "context: {context}\n",
        "temperature=0.0\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"context\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm,retriever=db.as_retriever(search_kwargs={'k':2}), chain_type_kwargs={\"prompt\": qa_prompt}, return_source_documents=True)\n",
        "result = qa_chain({\"question\": myquery, \"context\": myapp})\n",
        "\n",
        "\n",
        "print(qa_chain)\n",
        "\n",
        "\n",
        "res=llm.invoke(prompt)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "Uy911136d4SJ",
        "outputId": "fe52d982-a057-4382-caa6-06ebb0d10aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'myquery' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0882566bf15d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mqa_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqa_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_source_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmyquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmyapp\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_chain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'myquery' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E59Y6cWbd4VA",
        "outputId": "c169788d-c9bc-4940-86d8-ecf4246f7321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**æ‚¨å¥½ï¼æˆ‘æ˜¯å°ç£é‡‘èç›£ç£ç®¡ç†å§”å“¡æœƒçš„ã€Œçª—å£ã€ï¼Œæˆ‘å°ˆé–€ç‚ºæ‚¨è§£ç­”æœ‰é—œç›£ç†ç§‘æŠ€ (SupTech) çš„ç›¸é—œå•é¡Œã€‚**\n",
            "\n",
            "**SuperTech çš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ**\n",
            "\n",
            "SuperTech æ˜¯æŒ‡åˆ©ç”¨æ•¸ç¢¼æŠ€è¡“å’Œç¶²è·¯æŠ€è¡“ä¾†å»ºç«‹å’Œç‡Ÿé‹é‡‘èç”¢å“å’Œæœå‹™çš„é ˜åŸŸã€‚\n",
            "\n",
            "**SuperTech çš„ç›£ç†ç¯„åœæ˜¯ä»€éº¼ï¼Ÿ**\n",
            "\n",
            "SuperTech çš„ç›£ç†ç¯„åœæ¶µè“‹ä½†ä¸é™æ–¼ï¼š\n",
            "\n",
            "* ç”¢å“è¨­è¨ˆèˆ‡é–‹ç™¼\n",
            "* äº¤æ˜“è™•ç†\n",
            "* æŠ•è³‡ç®¡ç†\n",
            "* å®¢æˆ¶æœå‹™\n",
            "* å®‰å…¨èˆ‡åˆè¦æ€§\n",
            "\n",
            "**SuperTech çš„ç›£ç†ç¾©å‹™æ˜¯ä»€éº¼ï¼Ÿ**\n",
            "\n",
            "SuperTech çš„ç›£ç†ç¾©å‹™åŒ…æ‹¬ï¼š\n",
            "\n",
            "* å®šæœŸç›£æ§ç”¢å“å’Œæœå‹™çš„è¨­è¨ˆèˆ‡é–‹ç™¼éç¨‹\n",
            "* å®šæœŸç›£æ§äº¤æ˜“è™•ç†çš„éç¨‹\n",
            "* å®šæœŸç›£æ§æŠ•è³‡ç®¡ç†çš„éç¨‹\n",
            "* å®šæœŸç›£æ§å®¢æˆ¶æœå‹™çš„éç¨‹\n",
            "* å®šæœŸç›£æ§å®‰å…¨èˆ‡åˆè¦æ€§çš„ç‹€æ³\n",
            "* æ¡å–é©ç•¶çš„æªæ–½ä¾†ç¢ºä¿ç”¢å“å’Œæœå‹™å®‰å…¨èˆ‡åˆè¦\n",
            "\n",
            "**å¦‚ä½•éµå®ˆ SuperTech çš„ç›£ç†ç¾©å‹™ï¼Ÿ**\n",
            "\n",
            "SuperTech çš„ç›£ç†ç¾©å‹™å¯ä»¥é€éä»¥ä¸‹æ–¹å¼å¯¦ç¾ï¼š\n",
            "\n",
            "* ç¢ºä¿ç”¢å“å’Œæœå‹™çš„è¨­è¨ˆèˆ‡é–‹ç™¼éç¨‹ç¬¦åˆå®‰å…¨èˆ‡åˆè¦æ¨™æº–\n",
            "* å»ºç«‹åš´æ ¼çš„äº¤æ˜“è™•ç†æµç¨‹\n",
            "* å»ºç«‹å®Œå–„çš„æŠ•è³‡ç®¡ç†åˆ¶åº¦\n",
            "* å»ºç«‹åš´æ ¼çš„å®¢æˆ¶æœå‹™ç³»çµ±\n",
            "* å»ºç«‹å®Œå–„çš„å®‰å…¨èˆ‡åˆè¦æ€§ç®¡ç†ç³»çµ±\n",
            "\n",
            "**å¦‚ä½•èˆ‡ SuperTech çš„ç›£ç†ç›¸é—œè¯ç¹«ï¼Ÿ**\n",
            "\n",
            "æ‚¨å¯ä»¥é€éä»¥ä¸‹æ–¹å¼èˆ‡ SuperTech çš„ç›£ç†ç›¸é—œè¯ç¹«ï¼š\n",
            "\n",
            "* é‡‘ç®¡æœƒç¶²ç«™ä¸Šçš„è³‡è¨Šä¸­å¿ƒ\n",
            "* é‡‘ç®¡æœƒè¾¦ç†çš„ç›£ç†å ±å‘Š\n",
            "* é‡‘ç®¡æœƒè¾¦ç†çš„å…¬é–‹æœƒè­°\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZ-0kH0fd4XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjy8zQacd4Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v82g7Zp8-I-m"
      },
      "source": [
        "## Test Library Setup\n",
        "\n",
        "Next, let's create our test \"library.\"\n",
        "\n",
        "For simplicity's sake, let's say that our \"library\" is simply a **nested directory of `.epub` files**. We can easily see this solution generalizing to, say, a Calibre library with a `metadata.db` database file. We'll leave that extension as an exercise for the reader. ğŸ˜‡\n",
        "\n",
        "Let's pull two `.epub` files from [Project Gutenberg](https://www.gutenberg.org/) for our library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie45lzO0-I-n"
      },
      "outputs": [],
      "source": [
        "!mkdir -p \".test/library/jane-austen\"\n",
        "!mkdir -p \".test/library/victor-hugo\"\n",
        "!wget https://www.gutenberg.org/ebooks/1342.epub.noimages -O \".test/library/jane-austen/pride-and-prejudice.epub\"\n",
        "!wget https://www.gutenberg.org/ebooks/135.epub.noimages -O \".test/library/victor-hugo/les-miserables.epub\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYp-HAmN-I-n"
      },
      "source": [
        "## RAG with LlamaIndex\n",
        "\n",
        "RAG with LlamaIndex, at its core, consists of the following broad phases:\n",
        "\n",
        "1. **Loading**, in which you tell LlamaIndex where your data lives and how to\n",
        "   load it;\n",
        "2. **Indexing**, in which you augment your loaded data to facilitate querying, e.g. with vector embeddings;\n",
        "3. **Querying**, in which you configure an LLM to act as the query interface for\n",
        "   your indexed data.\n",
        "\n",
        "This explanation only scratches at the surface of what's possible with\n",
        "LlamaIndex. For more in-depth details, I highly recommend reading the\n",
        "[\"High-Level Concepts\" page of the LlamaIndex\n",
        "documentation](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8QV16yJ-I-o"
      },
      "source": [
        "### Loading\n",
        "\n",
        "Naturally, let's start with the **loading** phase.\n",
        "\n",
        "I mentioned before that LlamaIndex is designed specifically for RAG. This\n",
        "immediately becomes obvious from its\n",
        "[`SimpleDirectoryReader`](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader.html)\n",
        "construct, which âœ¨ **magically** âœ¨ supports a whole host of multi-model file\n",
        "types for free. Conveniently for us, `.epub` is in the supported set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "url8lH9n-I-o"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "loader = SimpleDirectoryReader(\n",
        "    input_dir=\"./.test/\",\n",
        "    recursive=True,\n",
        "    required_exts=[\".epub\"],\n",
        ")\n",
        "\n",
        "documents = loader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWiY0D4-I-p"
      },
      "source": [
        "`SimpleDirectoryReader.load_data()` converts our ebooks into a set of [`Document`s](https://docs.llamaindex.ai/en/stable/api/llama_index.core.schema.Document.html) for LlamaIndex to work with.\n",
        "\n",
        "One important thing to note here is that the documents **have not been chunked at this stage** -- that will happen during indexing. Read on..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mDtpTdn-I-p"
      },
      "source": [
        "### Indexing\n",
        "\n",
        "Next up after **loading** the data is to **index** it. This will allow our RAG pipeline to look up the relevant context for our query to pass to our LLM to **augment** their generated response. This is also where document chunking will take place.\n",
        "\n",
        "[`VectorStoreIndex`](https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index.html)\n",
        "is a \"default\" entrypoint for indexing in LlamaIndex. By default,\n",
        "`VectorStoreIndex` uses a simple, in-memory dictionary to store the indices, but\n",
        "LlamaIndex also supports [a wide variety of vector storage\n",
        "solutions](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores.html)\n",
        "for you to graduate to as you scale.\n",
        "\n",
        "<Tip>\n",
        "By default, LlamaIndex uses a chunk size of 1024 and a chunk overlap of\n",
        "20. For more details, see the [LlamaIndex\n",
        "documentation](https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies.html#chunk-sizes).\n",
        "</Tip>\n",
        "\n",
        "\n",
        "Like mentioned before, we'll use the\n",
        "[`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) to\n",
        "generate our embeddings. By default, [LlamaIndex uses\n",
        "OpenAI](https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html)\n",
        "(specifically `gpt-3.5-turbo`), which we'd like to avoid given our desire for a lightweight, locally-runnable end-to-end solution.\n",
        "\n",
        "Thankfully, LlamaIndex supports retrieving embedding models from Hugging Face through the convenient `HuggingFaceEmbedding` class, so we'll use that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkJoR1xo-I-p"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mwPjFGz-I-p"
      },
      "source": [
        "We'll pass that in to `VectorStoreIndex` as our embedding model to circumvent the OpenAI default behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK5u69zP-I-p"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=embedding_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuEPvBTQ-I-p"
      },
      "source": [
        "### Querying\n",
        "\n",
        "Now for the final piece of the RAG puzzle -- wiring up the query layer.\n",
        "\n",
        "We'll use Llama 2 for the purposes of this recipe, but I encourage readers to play around with different models to see which produces the \"best\" responses here.\n",
        "\n",
        "First let's start up the Ollama server. Unfortunately, there is no support in the [Ollama Python client](https://github.com/ollama/ollama-python) for actually starting and stopping the server itself, so we'll have to pop out of Python land for this.\n",
        "\n",
        "In a separate terminal, run: `ollama serve`. Remember to terminate this after we're done here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sohcJ3cQ-I-q"
      },
      "source": [
        "Now let's hook Llama 2 up to LlamaIndex and use it as the basis of our query engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqlmG5mZ-I-q"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "llama = Ollama(\n",
        "    model=\"llama2\",\n",
        "    request_timeout=40.0,\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llama)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca8Gyur8-I-q"
      },
      "source": [
        "## Final Result\n",
        "\n",
        "With that, our basic RAG librarian is set up and we can start asking questions about our library. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvSrYLOq-I-q",
        "outputId": "415cfcdf-ddc3-4c68-f454-cde83b798a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the context provided, there are two books available:\n",
            "\n",
            "1. \"Pride and Prejudice\" by Jane Austen\n",
            "2. \"Les MisÃ©rables\" by Victor Hugo\n",
            "\n",
            "The context used to derive this answer includes:\n",
            "\n",
            "* The file path for each book, which provides information about the location of the book files on the computer.\n",
            "* The titles of the books, which are mentioned in the context as being available for reading.\n",
            "* A list of words associated with each book, such as \"epub\" and \"notebooks\", which provide additional information about the format and storage location of each book.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"What are the titles of all the books available? Show me the context used to derive your answer.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9gYtakU-I-q",
        "outputId": "3a90cb94-0de9-4af7-900a-18dcd9d3cf3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The main character of 'Pride and Prejudice' is Elizabeth Bennet.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"Who is the main character of 'Pride and Prejudice'?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYqlu4AX-I-r"
      },
      "source": [
        "## Conclusion and Future Improvements\n",
        "\n",
        "We've demonstrated how to build a basic RAG-based \"librarian\" that runs entirely locally, even on Apple silicon Macs. In doing so, we've also carried out a \"grand tour\" of LlamaIndex and how it streamlines the process of setting up RAG-based applications.\n",
        "\n",
        "That said, we've really only scratched the surface of what's possible here. Here are some ideas of how to refine and build upon this foundation.\n",
        "\n",
        "### Forcing Citations\n",
        "\n",
        "To guard against the risk of our librarian hallucinating, how might we require that it provide citations for everything that it says?\n",
        "\n",
        "### Using Extended Metadata\n",
        "\n",
        "Ebook library management solutions like [Calibre](https://calibre-ebook.com/) create additional metadata for ebooks in a library. This can provide information such as publisher or edition that might not be readily available in the text of the book itself. How could we extend our RAG pipeline to account for additional sources of information that aren't `.epub` files?\n",
        "\n",
        "### Efficient Indexing\n",
        "\n",
        "If we were to collect everything we built here into a script/executable, the resulting script would re-index our library on each invocation. For our tiny test library of two files, this is \"fine,\" but for any library of non-trivial size this will very quickly become annoying for users. How could we persist the embedding indices and only update them when the contents of the library have meaningfully changed, e.g. new books have been added?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}